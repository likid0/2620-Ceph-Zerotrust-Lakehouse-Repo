//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4

== Introduction

This troubleshooting guide offers a practical, hands-on approach to diagnosing
and resolving issues within an IBM Storage Ceph cluster. By employing a 'break
and fix' methodology, you will begin with a healthy cluster that simulates
real-world challenges you must resolve. This approach ensures a practical learning experience and helps users confidently handle real-world troubleshooting scenarios.

== Checking the current state of the lab

If you are reading this doc, you should have your IBM Storage Ceph
Troubleshooting TechZone Lab up and running. If that is not the case, please go
to the IBM Storage Ceph Tech-Zone Collection and Order the troubleshooting Lab https://techzone.ibm.com/collection/64b92c8897187f0017773310)[TechZone Lab Access]

We must open a CLI terminal in our workstation machine and sudo to run the
lab commands as the `ROOT` user. The workstation has the required ceph client
RPMs and the CephX admin keys for our deployment so that
we can run most of the necessary commands for this lab from the workstation.

----
$ sudo -i
# ceph -s
  cluster:
    id:     09f357c6-b8d6-11ef-bbb7-02009a7a348a
    health: HEALTH_OK

  services:
    mon: 4 daemons, quorum ceph-node1-675b5683b75e66c49dc8f254,ceph-node2-675b5683b75e66c49dc8f254,ceph-node3-675b5683b75e66c49dc8f254,ceph-node4-675b5683b75e66c49dc8f254 (age 9h)
    mgr: ceph-node1-675b5683b75e66c49dc8f254.vadpyr(active, since 9h), standbys: ceph-node2-675b5683b75e66c49dc8f254.yuzazl
    mds: 1/1 daemons up, 1 standby
    osd: 12 osds: 12 up (since 9h), 12 in (since 9h)
    rgw: 1 daemon active (1 hosts, 1 zones)

  data:
    volumes: 1/1 healthy
    pools:   9 pools, 465 pgs
    objects: 250 objects, 456 KiB
    usage:   856 MiB used, 119 GiB / 120 GiB avail
    pgs:     465 active+clean

  io:
    client:   85 B/s rd, 0 op/s rd, 0 op/s wr
----

== Breaking our Ceph Cluster

To intentionally break the environment for troubleshooting purposes, we will
execute an ansible playbook designed to simulate real life issues in our
healthy cluster. 

We will use the `nohup` command, which allows the playbook to run in the background independently of the terminal session. Ensure you execute these commands from the workstation as root:

----
$ sudo -i
# nohup bash -c "ansible-playbook /root/scripts/break_and_fix1.yaml | tee -a /root/ansible.log" &
----

You can monitor the progress of the playbook by checking the Ansible log. When the log reaches the task with the message:

----
TASK [Don't stop this playbook; leave it running during the session]*
----

The Ansible playbook has completed its initial setup, and you can proceed to
the next steps, you can also check the logs of the playbook using the
/root/ansible.log file, here is an example of the output of the file when it
has finished the ansible playbook run:

----
# tail -f /root/ansible.log
TASK [Break and fix operation 2] ***********************************************
changed: [localhost]
...
TASK [Don't stop this playbook; leave it running during the session] ***********
----

From the same terminal or in a new one, we can investigate the current status
of our cluster, its quite a mess, lets take a closer look:

----
# ceph -s
  cluster:
    id:     09f357c6-b8d6-11ef-bbb7-02009a7a348a
    health: HEALTH_WARN
            Failed to apply 1 service(s): nvmeof.rbd
            Failed to place 1 daemon(s)
            2 failed cephadm daemon(s)
            1 filesystem is degraded
            1 MDSs report slow metadata IOs
            3 osds down
            Reduced data availability: 63 pgs inactive
            Degraded data redundancy: 236/814 objects degraded (28.993%), 47 pgs degraded, 309 pgs undersized

  services:
    mon: 4 daemons, quorum ceph-node1-675b5683b75e66c49dc8f254,ceph-node2-675b5683b75e66c49dc8f254,ceph-node3-675b5683b75e66c49dc8f254,ceph-node4-675b5683b75e66c49dc8f254 (age 9h)
    mgr: ceph-node1-675b5683b75e66c49dc8f254.vadpyr(active, since 9h), standbys: ceph-node2-675b5683b75e66c49dc8f254.yuzazl
    mds: 1/1 daemons up, 1 standby
    osd: 12 osds: 9 up (since 102s), 12 in (since 9h)

  data:
    volumes: 0/1 healthy, 1 recovering
    pools:   9 pools, 465 pgs
    objects: 256 objects, 457 KiB
    usage:   882 MiB used, 119 GiB / 120 GiB avail
    pgs:     13.548% pgs not active
             236/814 objects degraded (28.993%)
             209 active+undersized
             156 active+clean
             53  undersized+peered
             37  active+undersized+degraded
             10  undersized+degraded+peered

  io:
    client:   85 B/s rd, 23 KiB/s wr, 0 op/s rd, 23 op/s wr
----


[IMPORTANT]
====
From this point forward, the guide will help you fix the issues one by one. **We recommend that you first try resolving the cluster problems on your own.** If you get stuck, refer to this guide for assistance.
====



== Let the Fun Begin, Fixing our cluster issues:

We are going to work our way up from the Core fixing the issues one by one, so we will
begin with the OSDs, we can see from the `ceph -s` health reports that `3 osds down` are
marked down, and we have `63 pgs inactive`. As you already know
inactive PGs are a big issue as they are not accessible to the clients and IO
will freeze or error out when trying to access those PGs for reads or writes

NOTE: It's vital to notice that the 3 OSDs are `DOWN` but not `OUT` as you
can see from this line of output in the ceph -s command, where 12 osds are
still in the cluster although only 9 are up `osd: 12 osds: 9 up (since 102s), 12 in (since 9h)`

NOTE: Why is this important? As you already know, Ceph/Rados won't take action and
start the recovery of the data for the failed OSDs until the OSDs are declared
DOWN + OUT, the time it takes for an OSD to be declared out is configurable with the
config parameter `mon_osd_down_out_interval` by default set to 10 minutes.

[TIP]
====
Important Ceph config parameters related to OSDs being DOWN, OUT.
[cols="1,3", options="header"]
|===
| Parameter | Description

| osd_heartbeat_grace
| Specifies the time (in seconds) that an OSD will wait for heartbeats from its peers before considering them down. The default is typically 20 seconds.

| mon_osd_down_out_interval
| Determines the time (in seconds) the monitor waits after an OSD is marked down before marking it out. This allows the cluster to begin data rebalancing. The default is usually 600 seconds (10 minutes).

| mon_osd_min_down_reporters
| Sets the minimum number of OSDs that must report another OSD as down before the monitor marks it down. This helps prevent false positives due to network glitches.

| mon_osd_report_timeout
| The time (in seconds) after which the monitor will mark an OSD as down if it hasn't received any reports from it.

| mon_osd_down_out_subtree_limit
| Sets the CRUSH subtree level (failure domain) at which the monitors will avoid marking OSDs as down or out if they become unreachable.
|=== 
====

Let’s get a `ceph health detail` output to have a deeper look into the issues we are facing

----
# ceph health detail | grep osd
HEALTH_WARN Failed to apply 1 service(s): nvmeof.rbd; Failed to place 1 daemon(s); 2 failed cephadm daemon(s); 1 filesystem is degraded; 1 MDSs report slow metadata IOs; 3 osds down; Reduced data availability: 63 pgs inactive; Degraded data redundancy: 236/814 objects degraded (28.993%), 47 pgs degraded, 309 pgs undersized
    daemon osd.1 on ceph-node1-675b5683b75e66c49dc8f254 is in error state
[WRN] OSD_DOWN: 3 osds down
    osd.1 (root=default,host=ceph-node1-675b5683b75e66c49dc8f254) is down
    osd.0 (root=default,host=ceph-node2-675b5683b75e66c49dc8f254) is down
    osd.11 (root=default,host=ceph-node3-675b5683b75e66c49dc8f254) is down

# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                                     STATUS  REWEIGHT  PRI-AFF
-1         0.11755  root default
-5         0.02939      host ceph-node1-675beb1fb75e66c49dc8f35b
 1    hdd  0.00980          osd.1                                   down   1.00000  1.00000
 3    hdd  0.00980          osd.3                                     up   1.00000  1.00000
 5    hdd  0.00980          osd.5                                     up   1.00000  1.00000
-3         0.02939      host ceph-node2-675beb1fb75e66c49dc8f35b
 0    hdd  0.00980          osd.0                                   down   1.00000  1.00000
 2    hdd  0.00980          osd.2                                     up   1.00000  1.00000
 4    hdd  0.00980          osd.4                                     up   1.00000  1.00000
-7         0.02939      host ceph-node3-675beb1fb75e66c49dc8f35b
 6    hdd  0.00980          osd.6                                     up   1.00000  1.00000
 8    hdd  0.00980          osd.8                                     up   1.00000  1.00000
10    hdd  0.00980          osd.10                                    up   1.00000  1.00000
-9         0.02939      host ceph-node4-675beb1fb75e66c49dc8f35b
 7    hdd  0.00980          osd.7                                     up   1.00000  1.00000
 9    hdd  0.00980          osd.9                                     up   1.00000  1.00000
11    hdd  0.00980          osd.11                                  down   1.00000  1.00000
----

If we check the PG status, we can see that we have PGs in an inactive state. This situation occurs because they have a single OSD in the acting set. Our pool is configured with a replication size of 3 and a minimum size (min_size) of 2 copies. Once the available copies for a PG drop below the min_size (2 in this case), the cluster will stop client I/O to ensure data consistency and integrity.
----
# ceph pg dump pgs_brief | grep -v active
dumped pgs_brief
PG_STAT  STATE                       UP          UP_PRIMARY  ACTING      ACTING_PRIMARY
3.f7              undersized+peered         [2]           0         [2]               0
3.f6              undersized+peered         [4]           4         [4]               4
3.d1              undersized+peered         [5]           0         [5]               0
3.ae              undersized+peered         [2]           2         [2]               2
3.ab              undersized+peered         [7]           0         [7]               0
----

The output shows PGs in an undersized+peered state, indicating that these PGs have fewer replicas than the configured size. In other words, the cluster does not have the required number of OSDs actively participating in replication. Although the PGs are peered (the cluster knows which OSDs should hold the data), they remain undersized because not all required OSDs are up and running.

----
# ceph osd pool ls detail | grep 'pool 3'
pool 3 'cephfs.cephfs.data' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 256 pgp_num 256 autoscale_mode on last_change 84 lfor 0/0/78 flags hashpspool,bulk stripe_width 0 application cephfs read_balance_score 1.60
----

This confirms that the pool has a size of 3 and min_size of 2. When the number of available replicas falls below 2, client I/O stops on those PGs.

----
# ceph pg map 3.f7
osdmap e205 pg 3.f7 (3.f7) -> up [3] acting [3]
----

This shows that PG 3.f7 is currently mapped to OSD 3 both in the up and acting sets. Because it’s a single OSD, it’s undersized for a replication size of 3. The PG is operational on that single OSD, but it cannot serve client I/O because it does not meet the minimum required replicas.

TIP: you can get a detailed status of a specific PG using the query command `# ceph pg 3.f7 query`

[TIP]
====
Here is a table with the most important PG states that can help you understand the differences:
[cols="1,3", options="header"]
|===
| State | Description

| active+clean 
| PG is active and all data replicas are synchronized. Indicates a healthy state; data is fully replicated and accessible.

| active+degraded 
| PG is active but missing one or more replicas. Data is accessible, but redundancy is reduced; recovery is needed to restore replicas.

| active+undersized 
| PG has fewer OSDs in its acting set than the replication size. The cluster cannot maintain the desired replication level; there is a potential risk if additional failures occur.

| active+undersized+degraded 
| A combination of undersized and degraded states. The PG lacks sufficient replicas and some data is not fully replicated; immediate attention is required.

| active+recovering 
| PG is actively recovering missing or outdated replicas. Data redundancy is being restored; cluster performance may be impacted during recovery.

| active+recovery_wait 
| PG is waiting to start the recovery process. Recovery is pending, possibly due to resource constraints or configuration limits.

| peering 
| PG is determining the authoritative OSDs for data. This occurs during startup or after topology changes; it is a temporary state before becoming active.
|===

For more details, see the link:https://docs.ceph.com/en/reef/rados/operations/pg-states/[Ceph PG States Documentation].
====

=== Fixing Issue Number 1

Let's assess the status of the 3 OSDs that are down `osd.1 osd.3 osd.11`, let's
check with `ceph orch` the status of the daemons:

----
# ceph orch ps | grep osd | grep -v running
osd.1                                                    ceph-node1-675b5683b75e66c49dc8f254                    error            5m ago   9h        -    4096M  <unknown>         <unknown>     <unknown>
osd.0                                                    ceph-node2-675b5683b75e66c49dc8f254                    stopped          5m ago   9h        -    4096M  <unknown>         <unknown>     <unknown>
osd.11                                                   ceph-node3-675b5683b75e66c49dc8f254                    stopped          6m ago   9h        -    2610M  <unknown>         <unknown>     <unknown>
----

From this output, `osd.1` is in an error state, and `osd.0` and `osd.11` are stopped. If OSDs that should hold replicas are down or stopped, the PGs relying on them become undersized and possibly inactive for client I/O.

If the OSDs were stopped accidentally or due to a recent issue, we can try to restart them using `ceph orch`:

----
# ceph orch daemon start osd.0
Scheduled to start osd.0 on host 'ceph-node2-675beb1fb75e66c49dc8f35b'
# ceph orch daemon start osd.11
Scheduled to start osd.11 on host 'ceph-node4-675beb1fb75e66c49dc8f35b'

# ceph orch ps | grep -E '(osd.11|osd.0)'
osd.0                                                    ceph-node2-675beb1fb75e66c49dc8f35b                    running (3m)      3m ago  18m    12.0M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  fa5d359d92e4
osd.11                                                   ceph-node4-675beb1fb75e66c49dc8f35b                    running (3m)      3m ago  17m    12.2M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  7c4c2b5ae479
----

TIP: Although we are not using it in this lab, you can avoid data movement during maintenance periods using the following flags
[cols="1,1,2,2", options="header"]
|===
| Flag | Command | Purpose | Use Case

| noout
| # ceph osd set noout
| Prevents OSDs from being marked "out" if they go down.
| Use during short-term maintenance to avoid rebalancing and data movement.

| norebalance
| # ceph osd set norebalance
| Stops automatic data rebalancing across OSDs.
| Use during cluster maintenance to prevent background rebalancing, which can impact performance.

| nobackfill
| # ceph osd set nobackfill
| Prevents backfill operations when OSDs are added or come back online.
| Use when adding new nodes or reintroducing OSDs to control and delay backfilling until ready.

| norecover
| # ceph osd set norecover
| Disables recovery operations for degraded placement groups.
| Set temporarily during critical maintenance to minimize load, especially if recovery impacts I/O.
|===


Let’s check for inactive PGs
----
# ceph pg dump_stuck inactive
ok
----

It’s looking a bit better, there are currently no inactive PGs in the cluster so clients can access data without issues, if we check with ceph osd tree, `osd.1` is still down.
----
# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME                                     STATUS  REWEIGHT  PRI-AFF
-1         0.11755  root default
-5         0.02939      host ceph-node1-675beb1fb75e66c49dc8f35b
 1    hdd  0.00980          osd.1                                   down         0  1.00000
 3    hdd  0.00980          osd.3                                     up   1.00000  1.00000
 5    hdd  0.00980          osd.5                                     up   1.00000  1.00000
-3         0.02939      host ceph-node2-675beb1fb75e66c49dc8f35b
 0    hdd  0.00980          osd.0                                     up   1.00000  1.00000
 2    hdd  0.00980          osd.2                                     up   1.00000  1.00000
 4    hdd  0.00980          osd.4                                     up   1.00000  1.00000
-7         0.02939      host ceph-node3-675beb1fb75e66c49dc8f35b
 6    hdd  0.00980          osd.6                                     up   1.00000  1.00000
 8    hdd  0.00980          osd.8                                     up   1.00000  1.00000
10    hdd  0.00980          osd.10                                    up   1.00000  1.00000
-9         0.02939      host ceph-node4-675beb1fb75e66c49dc8f35b
 7    hdd  0.00980          osd.7                                     up   1.00000  1.00000
 9    hdd  0.00980          osd.9                                     up   1.00000  1.00000
11    hdd  0.00980          osd.11                                    up   1.00000  1.00000
----

Let’s try to restart `osd.1` just to check if this can be a quick fix to get it working again

----
# ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'ceph-node1-675beb1fb75e66c49dc8f35b'
----

If we do a refresh of the cephadm cache to just to be sure we have the latest information:

----
# ceph orch ps --refresh > /dev/null  && ceph orch ps | grep osd.1
osd.1                                                    ceph-node1-675beb1fb75e66c49dc8f35b                    unknown          11s ago  22m        -    4096M  <unknown>         <unknown>     <unknown>
----

So `osd.1`  is not starting. It remains in an unknown state, at this point, osd.1 has
been declared down and out of the cluster, so the data has been copied, so
fixing this OSD is not a priority at the moment. 

=== Fixing Issue Number 2

If we check the PG status, we still have undersized PGs; they are active, but
showing undersized+degraded

----
# ceph pg dump_stuck
PG_STAT  STATE                       UP          UP_PRIMARY  ACTING      ACTING_PRIMARY
2.8               active+undersized   [9,8,0,5]           9   [9,8,0,5]               9
2.9               active+undersized  [4,9,10,3]           4  [4,9,10,3]               4
2.a               active+undersized   [6,4,7,3]           6   [6,4,7,3]               6
2.c      active+undersized+degraded   [6,0,7,5]           6   [6,0,7,5]               6
2.3      active+undersized+degraded  [11,5,8,4]          11  [11,5,8,4]              11
2.d      active+undersized+degraded  [10,9,2,5]          10  [10,9,2,5]              10
2.0      active+undersized+degraded   [3,6,0,7]           3   [3,6,0,7]               3
2.e      active+undersized+degraded   [2,8,9,5]           2   [2,8,9,5]               2
2.2               active+undersized   [5,4,7,6]           5   [5,4,7,6]               5
2.f      active+undersized+degraded   [8,9,3,0]           8   [8,9,3,0]               8
2.1               active+undersized   [7,0,6,3]           7   [7,0,6,3]               7
2.7      active+undersized+degraded   [3,7,2,8]           3   [3,7,2,8]               3
2.6      active+undersized+degraded  [2,6,11,3]           2  [2,6,11,3]               2
2.5      active+undersized+degraded   [8,4,7,5]           8   [8,4,7,5]               8
2.4      active+undersized+degraded  [2,10,9,3]           2  [2,10,9,3]               2
2.b      active+undersized+degraded  [8,5,11,4]           8  [8,5,11,4]               8
----

To determine why a PG (Placement Group) is undersized, you need to understand
what that state means in the context of Ceph. An “undersized” PG indicates that
it does not have the whole number of replica copies that the pool requires. In
other words, the cluster cannot currently meet the configured replication or
erasure coding requirements for that PG set by the pool.

If you take a look at all the `undersized` PGs they all belong to the same pool, the pool ID
is the first number of the `PG ID`, so its `POOLID.PGNUM`, in this case, the pool ID
is 2, let's go ahead and check what is the replication schema configuration
For this pool, it seems also strange that we have 4 OSDs listed for each PG in the `UP` section of the output.

----
# ceph osd pool ls detail | grep "pool 2"
pool 2 'cephfs.cephfs.meta' replicated size 5 min_size 3 crush_rule 0 object_hash rjenkins pg_num 16 pgp_num 16 autoscale_mode on last_change 198 lfor 0/0/47 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs read_balance_score 2.24
----

Ok! so here is the issue, someone has set by mistake a replication scheme of 5,
so the pool requires five copies of the data, this is not possible in our cluster
because our failure domain for the pool is set to host, and we only have four
nodes in the cluster, so our maximum replication factor can only be 4:

----
# ceph osd crush rule list
replicated_rule
# ceph osd crush rule dump replicated_rule
{
    "rule_id": 0,
    "rule_name": "replicated_rule",
    "type": 1,
    "steps": [
        {
            "op": "take",
            "item": -1,
            "item_name": "default"
        },
        {
            "op": "chooseleaf_firstn",
            "num": 0,
            "type": "host"
        },
        {
            "op": "emit"
        }
    ]
}
----

Let's set the size and min_size to a count of 3 and 2.

----
# ceph osd pool set cephfs.cephfs.meta size 3
set pool 2 size to 3
# ceph osd pool set cephfs.cephfs.meta min_size 2
set pool 2 min_size to 2
----

Once we have change the replication schema values for the pool you can see that all our PGs are `active+clean`

----
# ceph pg dump_stuck
ok
# ceph -s | grep pgs
    pools:   9 pools, 465 pgs
    pgs:     465 active+clean
----

TIP: High-level differences between the Recovery, Backfill, and Rebalance processes
[cols="1,3,3,2", options="header"]
|===
| Operation | Description | Trigger | Impact

| Recovery 
| The process of restoring missing or outdated replicas to achieve the desired redundancy.
| Triggered when OSDs fail, go down, or become unreachable, causing some placement groups (PGs) to lose replicas.
| Can impact I/O performance due to additional read/write operations as the cluster replicates data to restore full redundancy.

| Backfill 
| The process of populating new or returned OSDs with the appropriate data to achieve a balanced data distribution.
| Triggered when new OSDs are added, or previously failed OSDs come back online after being marked "out."
| Can cause higher I/O load as data is moved to the OSDs that need to be filled, potentially impacting client I/O performance temporarily.

| Rebalance 
| The redistribution of data across OSDs to maintain an even utilization and performance profile throughout the cluster.
| Triggered by changes in cluster topology (e.g., adding or removing OSDs, changing CRUSH map rules) that affect the data placement.
| Generates additional data movement that can temporarily reduce performance, but ultimately aims for a more balanced and efficient cluster.
|===


=== Fixing Issue Number 3

Ok, our PGs are back to active clean; now let's go back to OSD.1 and see if we
can fix it, our cluster's current state

----
# ceph health detail
HEALTH_WARN Failed to apply 1 service(s): nvmeof.rbd; 3 failed cephadm daemon(s)
[WRN] CEPHADM_APPLY_SPEC_FAIL: Failed to apply 1 service(s): nvmeof.rbd
    nvmeof.rbd: Cannot find pool "rbd" for service nvmeof.rbd
[WRN] CEPHADM_FAILED_DAEMON: 3 failed cephadm daemon(s)
    daemon osd.1 on ceph-node1-675beb1fb75e66c49dc8f35b is in error state
    daemon rgw.objectgw.ceph-node2-675beb1fb75e66c49dc8f35b.cvkhtd on ceph-node2-675beb1fb75e66c49dc8f35b is in error state
    daemon nvmeof.rbd.ceph-node3-675beb1fb75e66c49dc8f35b.qdxrlt on ceph-node3-675beb1fb75e66c49dc8f35b is in error state
----

Let's check the container startup logs for the OSD using the cephadm command,
the `cephadm logs` command needs to be checked from the node where the OSD is running,
another way to get on what node a specific OSD is running

----
# ceph osd find osd.1 | grep host
    "host": "ceph-node1-675beb1fb75e66c49dc8f35b",
        "host": "ceph-node1-675beb1fb75e66c49dc8f35b",
----

We ssh into the ceph-node1

----
# ssh ceph-node1-675beb1fb75e66c49dc8f35b
----

Use the cephadm command to check the container startup logs, looking for the error or bad strings, in the hope we get some valuable info.

----
# cephadm logs --name osd.1 | grep -iE '(error|bad)'
Inferring fsid 759da2cc-b92d-11ef-bc4f-020012356cc9
Dec 13 08:57:11 ceph-node1-675beb1fb75e66c49dc8f35b ceph-759da2cc-b92d-11ef-bc4f-020012356cc9-osd-1[41399]: 2024-12-13T08:57:11.304+0000 7fcdcf7b2640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Dec 13 08:57:11 ceph-node1-675beb1fb75e66c49dc8f35b ceph-osd[41404]: monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Dec 13 08:57:11 ceph-node1-675beb1fb75e66c49dc8f35b ceph-759da2cc-b92d-11ef-bc4f-020012356cc9-osd-1[41399]: 2024-12-13T08:57:11.304+0000 7fcdce7b0640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Dec 13 08:57:11 ceph-node1-675beb1fb75e66c49dc8f35b ceph-osd[41404]: monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Dec 13 08:57:11 ceph-node1-675beb1fb75e66c49dc8f35b ceph-759da2cc-b92d-11ef-bc4f-020012356cc9-osd-1[41399]: 2024-12-13T08:57:11.305+0000 7fcdcefb1640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Dec 13 08:57:11 ceph-node1-675beb1fb75e66c49dc8f35b ceph-osd[41404]: monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
Dec 13 08:57:24 ceph-node1-675beb1fb75e66c49dc8f35b ceph-759da2cc-b92d-11ef-bc4f-020012356cc9-osd-1[41765]: 2024-12-13T08:57:24.284+0000 7f36996e7640 -1 monclient(hunting): handle_auth_bad_method server allowed_methods [2] but i only support [2]
----

From the message `handle_auth_bad_method server allowed_methods [2] but i only
support [2]` it seems there is an issue with the `cephx` authentication from the
OSD to the monitor

Ceph uses a system called “cephx” to securely identify and allow access to different parts of the cluster, like OSDs or monitors.
Each OSD has its own "cephx key" that it uses to prove who it is.
The monitors (mons) check this key to ensure the OSD is authorized to join the cluster.

TIP: For more information about CephX authentication, see the link:https://www.ibm.com/docs/en/storage-ceph/8.0?topic=components-ceph-authentication[Ceph Authentication Documentation].


Let's check if the OSD and Monitor Key entry for OSD.1 Match. From ceph-node1

----
# cat /var/lib/ceph/759da2cc-b92d-11ef-bc4f-020012356cc9/osd.1/keyring
[osd.1]
key = AQAD9VtnDcUOCRAAWKicP9ok/Z/BM7CGbSzDug==
----

Now let's check the MON keyring information; I exit ceph-node1 and go back to the workstation.

----
# ceph auth ls | grep osd.1
osd.10
osd.11
----

As you can see from the output, there is no key entry for OSD 1!!, it’s missing, this is the
reason why the OSD.1 daemon/service is not starting. We could try and
re-create/distribute the keys, but let's be pragmatic. All our PGs are active
even if undersized, and the data from OSD.1 has been copied through recovery to other OSDs once
it was declared out of the cluster, so let's move forward, and just recreate OSD.1, at this point it's safe and the fastest way to get it back online again.

[TIP]
====
In a Ceph cluster, each Object Storage Daemon (OSD) has its own dedicated directory to store data and metadata. This directory is essential for understanding how Ceph manages its storage backend.
The OSD directory is typically found in the /var/lib/ceph/osd/ path. Each directory is named based on the OSD ID assigned by the cluster.

Important files in the OSD dir:
*block:* A symbolic link pointing to the block device used by BlueStore.
*ceph_fsid:* Contains the unique identifier (UUID) for the Ceph cluster. This UUID matches the one displayed by the ceph -s command.
*keyring:* Stores the authentication key for the OSD to communicate with the monitor daemons (MONs).
*kv_backend:* Indicates the key-value store used, such as RocksDB, which manages metadata like object-to-disk mappings.
*unit.*: Files related with the systemd unit and the starting of the OSD service
====



Before we delete the OSD, let's get some information, like the underlying media
it's using, here are three different ways to achieve the information.

----
# ceph device ls | grep osd.1
07a7-402873cd-5da3-4  ceph-node3-675bf708cc0dca378231ef22:vdf  osd.10                                                        
07a7-8024e374-6e0b-4  ceph-node1-675bf708cc0dca378231ef22:vdd  osd.1                                                         
07a7-d972df4e-6cc1-4  ceph-node4-675bf708cc0dca378231ef22:vdf  osd.11

# ceph osd metadata 1 | grep device
    "bluefs_single_shared_device": "1",
    "bluestore_bdev_devices": "vdd",
    "default_device_class": "hdd",
    "device_ids": "vdd=07a7-8024e374-6e0b-4",
    "device_paths": "vdd=/dev/disk/by-path/pci-0000:00:09.0",
    "devices": "vdd",

# ceph orch device ls | grep vdd
ceph-node1-67628e99e82f4213d363447b  /dev/vdd  hdd   02d7-b81fcb20-fd60-4  10.0G  No         30m ago    Has a FileSystem, Insufficient space (<10 extents) on vgs, LVM detected  
----

So the device is named vdd in ceph-node1, and we can double-check that cephadm
uses LVM with the OSD, so it creates a PV/VG/LV per OSD; this is the simple
an example, where we have all our bluestore components, Data(block),
DB(block.db) and WAL(block.wal) in a single device, as you know, the DB and WAL can be on a diffent device, like for
example NVMe flash media devices to improve performance.

----
# ssh ceph-node1 lsblk | grep -C 2 vdd
vdb                                                                                                   252:16   0   396K  0 disk
vdc                                                                                                   252:32   0    44K  0 disk
vdd                                                                                                   252:48   0    10G  0 disk
└─ceph--925a4ac8--c3d7--4c85--8167--ec7293f1c1e8-osd--block--bfad4c45--836b--4652--a37d--ee6d1d809f42 253:0    0    10G  0 lvm

# ssh ceph-node1 "pvs ; vgs ; lvs"
  PV         VG                                        Fmt  Attr PSize   PFree
  /dev/vdd   ceph-925a4ac8-c3d7-4c85-8167-ec7293f1c1e8 lvm2 a--  <10.00g    0
  /dev/vde   ceph-4c55e01c-3ad1-4f7d-aa5c-28faf080cc06 lvm2 a--  <10.00g    0
  /dev/vdf   ceph-7404dc47-dc52-4953-8448-d218e37e4ac7 lvm2 a--  <10.00g    0
  VG                                        #PV #LV #SN Attr   VSize   VFree
  ceph-4c55e01c-3ad1-4f7d-aa5c-28faf080cc06   1   1   0 wz--n- <10.00g    0
  ceph-7404dc47-dc52-4953-8448-d218e37e4ac7   1   1   0 wz--n- <10.00g    0
  ceph-925a4ac8-c3d7-4c85-8167-ec7293f1c1e8   1   1   0 wz--n- <10.00g    0
  LV                                             VG                                        Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  osd-block-e5395b87-01f9-49e3-a7e0-88c4c10a55dc ceph-4c55e01c-3ad1-4f7d-aa5c-28faf080cc06 -wi-ao---- <10.00g
  osd-block-f37ad217-4750-492d-b819-4284d1ee0127 ceph-7404dc47-dc52-4953-8448-d218e37e4ac7 -wi-ao---- <10.00g
  osd-block-bfad4c45-836b-4652-a37d-ee6d1d809f42 ceph-925a4ac8-c3d7-4c85-8167-ec7293f1c1e8 -wi-a----- <10.00g
----

We will now remove the device using the cephadm orchestration for OSD removal
that makes the procedure straightforward, we use `ceph orch osd rm OSD.ID` we
are adding the `--zap` option, so cephadm takes care of zapping the disks
during the remove(removing all data and headers from the disk)

----
# ceph orch osd rm 1 --zap

# ceph orch osd rm status
OSD  HOST                                 STATE                    PGS  REPLACE  FORCE  ZAP   DRAIN STARTED AT
1    ceph-node1-675bf708cc0dca378231ef22  done, waiting for purge    0  False    False  True

# ceph orch osd rm status
No OSD remove/replace operations reported
----

NOTE: If you don't add the `--zap` option, the OSD won't be automatically added
back to the system, and you will need to run the `ceph orch device zap` command
to be able to re-use the drive as an OSD.

OSD 1 should get automatically re-configured into the cluster because we have
our OSD spec configured to do so, we can check with `ceph orch ls osd --export`
we can see that in the spec for the section `data_devices`, we have the filter
`all: true` This means that cephadm will use all drives available to a max
`limit: 3` because we have zapped our OSD.1 it will show as available again to
cephadm and will re-deploy it

----
# ceph orch ls osd
NAME                       PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
osd.all-available-devices              12  5m ago     4h   label:osd

# ceph orch ls osd --export
service_type: osd
service_id: all-available-devices
service_name: osd.all-available-devices
placement:
  label: osd
spec:
  data_devices:
    all: true
    limit: 3
  filter_logic: AND
  objectstore: bluestore
----

After a couple of minutes, I have all the OSDs up and runnin,g including osd.1
that has been re-deployed for us:

----
# ceph orch ps --daemon-type osd
NAME    HOST                                 PORTS  STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION           IMAGE ID      CONTAINER ID
osd.0   ceph-node2-67628e99e82f4213d363447b         running (18m)     7m ago   4h    95.1M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  4f9162246477
osd.1   ceph-node1-67628e99e82f4213d363447b         running (5m)      4m ago   5m    83.5M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  3cf13170ac67
osd.2   ceph-node2-67628e99e82f4213d363447b         running (4h)      7m ago   4h    97.2M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  8e8326515a9b
osd.3   ceph-node1-67628e99e82f4213d363447b         running (4h)      4m ago   4h     111M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  71fc5a8c2eaa
osd.4   ceph-node2-67628e99e82f4213d363447b         running (4h)      7m ago   4h    97.1M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  050e2b9d13e5
osd.5   ceph-node1-67628e99e82f4213d363447b         running (4h)      4m ago   4h     100M    4096M  18.2.1-262.el9cp  1a4ea7f62a89  4267d34f6897
osd.6   ceph-node3-67628e99e82f4213d363447b         running (4h)      8s ago   4h    93.4M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  e6c64bde0a2c
osd.7   ceph-node4-67628e99e82f4213d363447b         running (4h)      8m ago   4h     104M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  a9e6de957ef8
osd.8   ceph-node3-67628e99e82f4213d363447b         running (4h)      8s ago   4h    98.0M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  ee49762f6aa3
osd.9   ceph-node4-67628e99e82f4213d363447b         running (4h)      8m ago   4h    90.5M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  0a64463ece4d
osd.10  ceph-node3-67628e99e82f4213d363447b         running (17m)     8s ago   4h    98.2M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  8cb27ac2f89f
osd.11  ceph-node4-67628e99e82f4213d363447b         running (4h)      8m ago   4h    94.8M    2269M  18.2.1-262.el9cp  1a4ea7f62a89  7fffb9ce1638

# ceph osd tree | grep -B 2 osd.1
-1         0.11755  root default
-5         0.02939      host ceph-node1-67628e99e82f4213d363447b
 1    hdd  0.00980          osd.1                                     up   1.00000  1.00000
----

All osds are up and in:

----
# ceph -s | grep osd
    osd: 12 osds: 12 up (since 7m), 12 in (since 7m)
----

=== Fixing Issue Number 4

With all OSDs fixed, we can move to our next issue, let's see what problems we
have in the cluster

----
# ceph health detail
HEALTH_WARN Failed to apply 1 service(s): nvmeof.rbd; 2 failed cephadm daemon(s)
[WRN] CEPHADM_APPLY_SPEC_FAIL: Failed to apply 1 service(s): nvmeof.rbd
    nvmeof.rbd: Cannot find pool "rbd" for service nvmeof.rbd
[WRN] CEPHADM_FAILED_DAEMON: 2 failed cephadm daemon(s)
    daemon rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo on ceph-node2-67628e99e82f4213d363447b is in error state
    daemon nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl on ceph-node3-67628e99e82f4213d363447b is in error state
----

Let's look at the RGW Object GW issue: `daemon rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo on ceph-node2-67628e99e82f4213d363447b is in error state`

----
# ceph orch ps | grep rgw
rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo  ceph-node2-67628e99e82f4213d363447b  *:8080            error             2m ago   4h        -        -  <unknown>         <unknown>     <unknown>
----

As the container is not starting, we will need to jump into the node where it's
trying to start and failing and take a look at the logs for this kind of
an error where the container doesn't start and the systemd unit is failing; it's a
It is a good idea to start by using the `cephadm logs --name` command; the name has to
be the name of the daemon running on the node in our example
`rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo``

----
# ssh ceph-node2

# cephadm logs --name  rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b systemd[1]: Started Ceph rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo for c52a9792-bd23-11ef-bd85-0200f67a348a.
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: deferred set uid:gid to 167:167 (ceph:ceph)
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: ceph version 18.2.1-262.el9cp (4857b2aad4c3aaa8ff58e0b60396fa6ab731f9ff) reef (stable), process radosgw, pid 2
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: framework: beast
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: framework conf key: port, val: 8080
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: init_numa not setting numa affinity
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: rgw main: ERROR: could not find zone (nozone)
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: rgw main: ERROR: failed to start notify service ((2) No such file or directory
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: rgw main: ERROR: failed to init services (ret=(2) No such file or directory)
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b ceph-c52a9792-bd23-11ef-bd85-0200f67a348a-rgw-objectgw-ceph-node2-67628e99e82f4213d363447b-gsisdo[85098]: 2024-12-18T13:42:02.461+0000 7f97ca526880 -1 Couldn't init storage provider (RAD>
Dec 18 13:42:02 ceph-node2-67628e99e82f4213d363447b radosgw[85102]: Couldn't init storage provider (RADOS)
----

Ok so we need to have basic knowled of the Object Gateway to understand the
error, but basically we can see that it's not able to find a zone called
`nozone`, snippet: `rgw main: ERROR: could not find zone (nozone)` , when the
RGW service starts is going to look certain pools that are start with the name
of the zone:

----
# ceph osd lspools | grep rgw
6 .rgw.root
7 default.rgw.log
8 default.rgw.control
9 default.rgw.meta
----

So, in the previous output, the name of our zone would be `default`; the problem
that we see in the logs and why the RGW service is not starting because
the RGW thinks he belongs to the `nozone` zone instead of `default`, so when it
tries to go and find the pools required to start with a name like
`nozone.rgw.log`, for example,RGW can't find the pools required and fails with the error
`radosgw[85102]: Couldn't init storage provider (RADOS)`, 

So first we need to check in the ceph config for the RGW service what zone is configured:

----
# ceph config dump | grep rgw
client.rgw                                                                                                advanced  rgw_zone                               nozone                                                                                                      *
client.rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo                                            basic     rgw_frontends                          beast port=8080                                                                                             *
----

Ok, so here is the issue, someone by mistake has configured all RGW services that
start with client.rgw to be part of the `nozone` zone; we need to change it to
default:

----
# ceph config set client.rgw rgw_zone default
# ceph config get client.rgw rgw_zone
default
----

Let's restart the RGW service so it uses the new zone we have configured. 

----
# ceph orch daemon restart rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo
Scheduled to restart rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo on host 'ceph-node2-67628e99e82f4213d363447b'
# ceph orch ps --daemon-type rgw
NAME                                                     HOST                                 PORTS   STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION           IMAGE ID      CONTAINER ID
rgw.objectgw.ceph-node2-67628e99e82f4213d363447b.gsisdo  ceph-node2-67628e99e82f4213d363447b  *:8080  running (19s)    14s ago   6h    75.7M        -  18.2.1-262.el9cp  1a4ea7f62a89  adf293824acc
# ceph -s | grep rgw
    rgw: 1 daemon active (1 hosts, 1 zones)
# curl http://ceph-node2-67628e99e82f4213d363447b:8080
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
----

=== Fixing Issue Number 5

Great! another problem is fixed, let's see what else we have

----
# ceph health detail
HEALTH_WARN Failed to apply 1 service(s): nvmeof.rbd; 1 failed cephadm daemon(s)
[WRN] CEPHADM_APPLY_SPEC_FAIL: Failed to apply 1 service(s): nvmeof.rbd
    nvmeof.rbd: Cannot find pool "rbd" for service nvmeof.rbd
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl on ceph-node3-67628e99e82f4213d363447b is in error state
----

The first error seems straightforward forward thanks `ceph health` for giving us such
a clear error message

----
# ceph osd lspools
1 rbdpool
2 cephfs.cephfs.meta
3 cephfs.cephfs.data
4 .nfs
5 .mgr
6 .rgw.root
7 default.rgw.log
8 default.rgw.control
9 default.rgw.meta
----

So we have a pool called `rbdpool` but not a pool named `rbd`, our NVMeoF
service seems to be configured with the `rbd` pool as the default, let's check:

----
# ceph orch ls nvmeof --export | grep pool
  pool: rbd
----

Ok so this is the culprit, let's create the `rbd` pool it's looking for

----
# ceph osd pool create rbd 32 32 replicated
pool 'rbd' created
# ceph osd pool application enable rbd rbd
----

We can give the ceph health detail command a couple of minutes to refresh and
remove the pool error

----
# ceph health detail
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl on ceph-node3-67628e99e82f4213d363447b is in error state
----

=== Fixing Issue Number 6

Great!, we only have one WARNING left! 

`daemon nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl on ceph-node3-67628e99e82f4213d363447b is in error state` , here I will proceed in the same way, ssh into  eph-node3 and check the container startup logs

----
# ssh ceph-node3

# cephadm logs --name nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl
....
[1]: ceph-c52a9792-bd23-11ef-bd85-0200f67a348a@nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl.service: Scheduled restart job, restart counter is at 3.
[1]: Stopped Ceph nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl for c52a9792-bd23-11ef-bd85-0200f67a348a.
[1]: Starting Ceph nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl for c52a9792-bd23-11ef-bd85-0200f67a348a...
186]: Trying to pull cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:8-8-8-8-8...
186]: Error: initializing source docker://cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:8-8-8-8-8: reading manifest 8-8-8-8-8 in cp.icr.io/cp/ibm-ceph/nvmeof-rhel9: manifest unknown
[1]: ceph-c52a9792-bd23-11ef-bd85-0200f67a348a@nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl.service: Control process exited, code=exited, status=125/n/a
----

Ok, so the container startup is complaining that it can't find the image
`cp.icr.io/cp/ibm-ceph/nvmeof-rhel9` with tag `8-8-8-8-8` in the IBM container
registry `Error: initializing source
docker://cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:8-8-8-8-8: reading manifest
8-8-8-8-8 in cp.icr.io/cp/ibm-ceph/nvmeof-rhel9: manifest unknow` this for sure
is a strange tag; let's check the systemd unit run script and see if the tag is
there, the long string after `/var/lib/ceph` is the `FSID` of the cluster so you
will need to replace it with the one in your deployment/TZ Lab:

----
# cat /var/lib/ceph/c52a9792-bd23-11ef-bd85-0200f67a348a/nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl/unit.run | grep nvmeof-rhel9
/bin/podman run --rm --ipc=host --stop-signal=SIGTERM --authfile=/etc/ceph/podman-auth.json --net=host --init --name ceph-c52a9792-bd23-11ef-bd85-0200f67a348a-nvmeof-rbd-ceph-node3-67628e99e82f4213d363447b-vuvzfl --pids-limit=-1 --ulimit memlock=-1:-1 --ulimit nofile=10240 --cap-add=SYS_ADMIN --cap-add=CAP_SYS_NICE -d --log-driver journald --conmon-pidfile /run/ceph-c52a9792-bd23-11ef-bd85-0200f67a348a@nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl.service-pid --cidfile /run/ceph-c52a9792-bd23-11ef-bd85-0200f67a348a@nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl.service-cid --cgroups=split -e CONTAINER_IMAGE=cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:8-8-8-8-8 -e NODE_NAME=ceph-node3-67628e99e82f4213d363447b -e CEPH_USE_RANDOM_NONCE=1 -v /var/lib/ceph/c52a9792-bd23-11ef-bd85-0200f67a348a/nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl/config:/etc/ceph/ceph.conf:z -v /var/lib/ceph/c52a9792-bd23-11ef-bd85-0200f67a348a/nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl/keyring:/etc/ceph/keyring:z -v /var/lib/ceph/c52a9792-bd23-11ef-bd85-0200f67a348a/nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl/ceph-nvmeof.conf:/remote-source/ceph-nvmeof/app/ceph-nvmeof.conf:z -v /var/lib/ceph/c52a9792-bd23-11ef-bd85-0200f67a348a/nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl/configfs:/sys/kernel/config -v /dev/hugepages:/dev/hugepages -v /dev/vfio/vfio:/dev/vfio/vfio -v /var/log/ceph/c52a9792-bd23-11ef-bd85-0200f67a348a:/var/log/ceph:z -v /etc/hosts:/etc/hosts:ro --mount type=bind,source=/lib/modules,destination=/lib/modules,ro=true cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:8-8-8-8-8
----

So we have confirmed it's using the tag: 8-8-8-8-8, but let's verify the available tag options in the IBM
Registry for this image. Back to the workstation node, I'm going to use the container registry credentials our running ceph cluster to authenticate with podman on the CLI, so I’m able to connect to the IBM registry and query all available tags using `podman search --list-tags`

----
# ceph config-key ls | grep registry
    "mgr/cephadm/registry_credentials",

# ceph config-key get mgr/cephadm/registry_credentials | jq .
{
  "url": "cp.icr.io/cp",
  "username": "cp",
  "password": "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE2NDMyOTExOTYsImp0aSI6ImE5MGY3NmMyMDI2NDRlMTViYmY5MWQxNjYxZWZlNTFjIn0.TjEwd_i-K7R21p60z16qIVIWW8ltVaso4QND-ICmJA0"
}

# podman login cp.icr.io/cp -u cp -p "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJJQk0gTWFya2V0cGxhY2UiLCJpYXQiOjE2NDMyOTExOTYsImp0aSI6ImE5MGY3NmMyMDI2NDRlMTViYmY5MWQxNjYxZWZlNTFjIn0.TjEwd_i-K7R21p60z16qIVIWW8ltVaso4QND-ICmJA0"
Login Succeeded!

# podman search --list-tags docker://cp.icr.io/cp/ibm-ceph/nvmeof-rhel9 
NAME                                TAG
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  0.0.5-12
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  0.0.5-3
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  0.0.5-8
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.2.13-4
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.2.16-27
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.2.16-8
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.2
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.3.3-10
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.3.3-14
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  1.3
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9  latest
----

So, as we can see 8-8-8-8-8 doesn't exist. This must be a mistake. Let's change
the ceph config for the NVMeoF image and use the `latest` tag

----
# ceph config-key get config/mgr/mgr/cephadm/container_image_nvmeof
cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:8-8-8-8-8
# ceph config-key set config/mgr/mgr/cephadm/container_image_nvmeof cp.icr.io/cp/ibm-ceph/nvmeof-rhel9:latest
----

We need to remove/delete the daemon so that cephadm will re-create the systemd unit
that starts the container with the right image tag

----
# ceph orch daemon rm nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl
Removed nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.vuvzfl from host 'ceph-node3-67628e99e82f4213d363447b'
----

If we wait for a couple of minutes, we will see how the nvmeof.rbd service we
have configured will create a new NVMeoF daemon/service, and this time it will
start without any issue as it's using the right container image tag

----
# ceph orch ps --daemon-type nvmeof
NAME                                                   HOST                                 PORTS             STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID
nvmeof.rbd.ceph-node3-67628e99e82f4213d363447b.niyxvx ceph-node3-67628e99e82f4213d363447b  *:5500,4420,8009  running (1m)     5m ago  79m    96.2M        -           86f83f6d8efb  353e94898694
----

Excelent job!!! with this final fix we have arrived at the end of the workshop,
Finally with the health of our cluster: `HEALTH_OK`

----
# ceph health detail
HEALTH_OK
----
