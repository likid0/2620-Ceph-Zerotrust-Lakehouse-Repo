//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4

== Introduction

[abstract]
In this hands‑on lab you will stand up a miniature—but fully functional—zero‑trust data lake that lives on *Ceph Object Gateway (RGW)* and is governed by the *Polaris* data‑catalog.  
You will automate the infrastructure with Terraform, ingest data with Spark, query it with Trino, and visualise it in Superset—while watching catalog‑level RBAC enforce least‑privilege at every step.

=== Why are we doing all this?
*A retail‑analytics story in four short scenes*

Imagine you are the data team for **FreshGoods**, a mid‑size grocery chain that
ships online orders from 40 local stores.  Every night each store uploads a
CSV “drop” to Ceph RGW containing the day’s *product movement* log:

* `product_id` • what was sold
* `category`   • department (dairy, produce, pantry …)
* `price`
* `quantity`
* `email`      • customer loyalty‑card e‑mail (PII we must protect)
* timestamps, etc.

Your mission: turn those raw files into insight the business can act on **by
9 a.m.**—without ever letting unauthorised eyes near the PII.

We will walk that journey in the lab, compressed into 90 minutes:

. **Ingest (Engineer ➜ Spark)**
  The *data‑engineer* persona lands last night’s CSV into an **Iceberg RAW
  table** `products_raw` using Spark.  Nothing is filtered or masked yet.

. **Curate & protect (Compliance ➜ Spark)**
  The *compliance* persona reads the RAW table, hashes the `email` column,
  calculates a `total = price * quantity`, and overwrites a clean
  **GOLD table** `products_gold`.
  They can still **read** RAW (audit duty) but only *they* can write GOLD.

. **Explore (Analyst ➜ Trino CLI)**
  The *analyst* persona checks row counts and quick aggregations from Trino,
  confirming the overnight ingest ran.

. **Visualise (Analyst ➜ Superset)**
  Finally the analyst refreshes a Superset dashboard showing *Category sales
  vs. previous day*—the chart the merchandisers see at roll‑call every
  morning.  The hashed emails never leave the lake; the analyst never sees PII.

The glue that enforces who can touch what is **Polaris** CatalogI.

By the end of the lab you’ll have a governed catalog exactly like a real
retailer might run—just shrunk to one bucket, two tables, and four personas so
we can see the whole life‑cycle in a single sitting.


=== What you will build

[source,mermaid]
----
flowchart TD
    subgraph Ceph_UI ["Ceph dashboard"]
        CD1["Login"] --> CD2["Create RGW account & root user"]
    end
    subgraph IaC ["Terraform"]
        TF1["ceph/ bucket + IAM"] --> TF2["polaris/ principals, grants, tables"]
    end
    subgraph Containers ["Runtime services"]
        DK1["docker compose up → Polaris • Trino • Jupyter • Superset"]
    end
    subgraph Pipeline ["Data pipeline"]
        direction LR
        E1["Engineer → products_raw (Spark)"]
        C1["Compliance → products_gold (Spark)"]
        A1["Analyst → query (Trino)"]
        A2["Analyst → dashboard (Superset)"]
    end
    CD2 --> TF1
    TF2 --> DK1
    DK1 --> E1 --> C1 --> A1 --> A2
----

=== Core skills you’ll practise

|===
| Pillar | You’ll learn to…

| *Storage*
| Create Ceph RGW buckets and IAM roles from the dashboard and Terraform.

| *Infrastructure‑as‑Code*
| Bootstrap all catalog objects (principals, grants, tables) with a single `terraform apply`.

| *Data Engineering*
| Use Spark to ingest CSV → Iceberg and to overwrite snapshots safely.

| *Governance / Security*
| Enforce role‑based access with Polaris tokens; watch failures when a role steps outside its lane.

| *Analytics*
| Query the same Iceberg tables from Trino CLI and render a Superset dashboard—all without additional credentials.

| *Lifecycle hygiene*
| Tear everything down cleanly with `./demo.sh destroy`.
|===

=== Lab flow at a glance

. *Login to Ceph Dashboard* – create RGW account & root user.  
. *Run Terraform (`ceph/`)* – wire bucket & IAM.  
. *Run Terraform (`polaris/`)* – create catalog, principals, RAW + GOLD tables.  
. *Start containers* – Polaris, Trino, Jupyter, Superset in one command.  
. *Engineer persona* – ingest `products_raw` in a Jupyter notebook.  
. *Compliance persona* – mask PII into `products_gold`.  
. *Analyst persona* – query GOLD via Trino and craft a Superset dashboard.  
. *Wrap‑up* – destroy resources and recap key take‑aways.

=== Estimated time

*90 minutes*

=== Provided for you

* Pre‑deployed IBM Storage Ceph RGW cluster.  
* Lab repository with Terraform code, notebooks, helper scripts, and a 200‑row sample CSV.  
* Pre‑generated Polaris tokens for the three personas.

== Checking the current state of the lab

If you are reading this doc, you should have your IBM Storage Ceph
Troubleshooting TechZone Lab up and running. If that is not the case, please go
to the IBM Storage Ceph Tech-Zone Collection and Order the troubleshooting Lab https://techzone.ibm.com/collection/64b92c8897187f0017773310)[TechZone Lab Access]

We must open a CLI terminal in our workstation machine and sudo to run the
lab commands as the `ROOT` user. The workstation has the required ceph client
RPMs and the CephX admin keys for our deployment so that
we can run most of the necessary commands for this lab from the workstation.

----
$ sudo -i
# ceph -s
  cluster:
    id:     09f357c6-b8d6-11ef-bbb7-02009a7a348a
    health: HEALTH_OK

  services:
    mon: 4 daemons, quorum ceph-node1-675b5683b75e66c49dc8f254,ceph-node2-675b5683b75e66c49dc8f254,ceph-node3-675b5683b75e66c49dc8f254,ceph-node4-675b5683b75e66c49dc8f254 (age 9h)
    mgr: ceph-node1-675b5683b75e66c49dc8f254.vadpyr(active, since 9h), standbys: ceph-node2-675b5683b75e66c49dc8f254.yuzazl
    osd: 12 osds: 12 up (since 9h), 12 in (since 9h)
    rgw: 1 daemon active (1 hosts, 1 zones)

  data:
    volumes: 1/1 healthy
    pools:   9 pools, 465 pgs
    objects: 250 objects, 456 KiB
    usage:   856 MiB used, 119 GiB / 120 GiB avail
    pgs:     465 active+clean

  io:
    client:   85 B/s rd, 0 op/s rd, 0 op/s wr
----

== Creating the required S3 IAM Account and Root Account User

=== Creating the IAM Account
radosgw-admin account create --account-name=analytics
=== Creating the Root User for the IAM Account
radosgw-admin user create --uid=analytics_root --display-name=root_analytics_user --account-id=RGW59183818904979875 --account-root  --access-key=demo --secret-key=demo
=== Creating the `polarisdemo` bucket
aws --profile polaris-root s3 mb s3://polarisdemo
=== Verification


== Configure and Run the Terraform Automation Code to Create Required Ceph RGW resources

Before we launch Spark, Trino, or Polaris we need a secure *landing zone* inside Ceph’s Object Gateway (RGW).
Rather than clicking through the Ceph Dashboard by hand, we’ll declare every bucket, user, and role in **Terraform**—an open-source “Infrastructure as Code” (IaC) tool that turns cloud resources into version-controlled files.

=== Why automate this step?

* **Consistency & repeatability** – Everyone in the team provisions the *exact* same resources , every time, with a single command.
* **Idempotence** – Running `terraform apply` tomorrow makes zero changes unless you changed the code.
* **Auditability** – All security-sensitive artifacts (bucket names, IAM policies, ARNs) can live in Git—no tribal knowledge locked in a UI click-path.

=== What the code does
[%header,cols="25,~"]
|===
| Block | Purpose

| *Variables (`*.tf` `variable` blocks)*
| Collect user-specific inputs such as the Ceph S3/STS endpoint, the credentials profile that can talk to RGW, and the bucket name that will back the Polaris catalog.

| *AWS provider configured for Ceph*
| Uses the standard `hashicorp/aws` provider but points its `s3`, `sts`, and `iam` endpoints to your Ceph cluster, and forces path-style S3 URLs so they work with RGW.

| *Bucket (data or resource)*
| Looks up—or optionally creates—the S3 bucket named in `var.bucket_name`.  The code is written with `data "aws_s3_bucket"` so it *reads* an already-provisioned bucket, but you can uncomment the `resource "aws_s3_bucket"` block to have Terraform create it instead.

| *IAM user `polaris/catalog/admin`*
| Creates a programmatic user that owns the catalog. Terraform outputs its *access key* and *secret key* so the next module (Polaris) can authenticate.

| *IAM role `polaris/catalog/client`*
| A role that the polaris catalogs assumes via `sts:AssumeRole` to vend a token
to the Query Engine(Spark, Trino) asking for access to a Table. It contains a single inline policy (`catalog_client_policy`) granting **only** `s3:*` on your warehouse bucket.  Principle of least privilege in action.

| *Outputs*
| After `terraform apply` you get:
  * `bucket_arn` – ARN of the warehouse bucket
  * `account_arn` – Ceph pseudo-account ID (used in later trust policies)
  * `location` – `s3://…` URI Polaris will register as its warehouse
  * `role_arn` – ARN of the client role
  * `admin_access_key` / `admin_secret_key` – keys for the admin user (the secret is marked *sensitive* so Terraform hides it in plan logs)
|===


=== Modify Variables
The Ceph Terraform Variables file we need to edit is located in our desktop
machine at `/root/terraform/ceph` with the name `terraform.tfvars`.

You only need to modify the RGW Account ID to match te Account ID you created
on your LAB Environment, the rest of the variables are already filled in for
you.

from the CLI you can get your Account ID with:

```
# radosgw-admin account list
[
    "RGW59183818904979875"
]
```

Then edit the /root/terraform/ceph/terraform.tfvars and modify the Account ID

# cat terraform.tfvars
# Ceph object-gateway (RGW) HTTPS endpoint, used for S3 **and** STS/IAM calls
ceph_endpoint       = "http://ceph-node2"

# Where Terraform’s AWS provider will read your access-key/secret-key pair
credentials_path    = "~/.aws/credentials"
credentials_profile = "polaris-root"

# Name of the bucket that will become Polaris’ warehouse
bucket_name         = "polarisdemo"

# The numerical “account ID” that Ceph assigns when you ran `radosgw-admin account create`
account_arn         = "RGW59183818904979875"  <<----- Modify this one!

# Object-storage URI the Polaris container should treat as its warehouse
location            = "s3://polarisdemo"


=== Run Terraform
With `terraform.tfvars` edited, you are ready to execute the automation.
All commands below assume you are **already on the lab workstation** and that
the code lives in `/root/terraform/ceph`.

[NOTE]
====
If you have never used Terraform before, think of the workflow as:

. *init* – download plugins and build a `.terraform` working directory
. *plan* – show what will change (dry‑run)
. *apply* – make it so (and save state in `terraform.tfstate`)
====

Open a terminal on the lab workstation and change to the module directory:

[source,bash]
----
# cd /root/terraform/ceph
----

Initialise the working directory (runs once per clone):

[source,bash]
----
# terraform init
----

Terraform downloads the **hashicorp/aws** provider, points it to your Ceph
endpoints, and prints *“Terraform has been successfully initialized!”* when
ready.

Preview the changes (optional but recommended):

[source,bash]
----
# terraform plan
----

You should see something like `Plan: 5 to add, 0 to change, 0 to destroy.`
Nothing is created yet—this is just a dry‑run so you can double‑check the
bucket name and account ID.

Apply the configuration:

[source,bash]
----
# terraform apply
----

Terraform re‑computes the plan and asks for confirmation.
Type `yes` (or add `-auto-approve` to skip the prompt) and watch the resources
appear.

When the run finishes you will see output similar to:

[source,plain]
----
Apply complete! Resources: 5 added, 0 changed, 0 destroyed.

Outputs:

account_arn      = "RGW59183818904979875"
admin_access_key = "POLARISADMINACCESSKEY"
admin_secret_key = (sensitive value)
bucket_arn       = "arn:aws:s3:::polarisdemo"
location         = "s3://polarisdemo"
role_arn         = "arn:aws:iam::RGW59183818904979875:role/polaris/catalog/client"
----

Copy these values—especially `admin_access_key`, `admin_secret_key`,
`location`, and `role_arn`—into the next Terraform module that provisions
Polaris.

==== What just happened?
* A warehouse bucket (`polarisdemo`) was confirmed (or created).
* An IAM user `polaris/catalog/admin` and its access keys were generated.
* A least‑privilege IAM role `polaris/catalog/client` with an inline S3 policy
  was created.
* Terraform wrote the resource IDs and ARNs to `terraform.tfstate` and echoed
  the key ones as outputs.

=== Verify 

From the terminal we can do a quick verification of the newly created Ceph
Resources:

Bucket:

----
# aws --profile polaris-root s3 ls
2025-06-24 08:57:39 polarisdemo
----

The User that polaris will use to assume the role:

----
# aws --profile polaris-root iam list-users
{
    "Users": [
        {
            "Path": "/polaris/catalog/",
            "UserName": "admin",
            "UserId": "a193f75b-3b62-4996-b8a2-5ba89161ddb2",
            "Arn": "arn:aws:iam::RGW59183818904979875:user/polaris/catalog/admin",
            "CreateDate": "2025-06-24T10:01:58.283604Z"
        }
    ]
}
----

The Role that Polaris will assume to get access to the S3 Resources:
----
# aws --profile polaris-root iam list-roles
{
    "Roles": [
        {
            "Path": "/polaris/catalog/",
            "RoleName": "client",
            "RoleId": "e8596597-1a55-4a44-9b20-364c0682a3a7",
            "Arn": "arn:aws:iam::RGW59183818904979875:role/polaris/catalog/client",
            "CreateDate": "2025-06-24T10:01:58.286Z",
            "AssumeRolePolicyDocument": {
                "Statement": [
                    {
                        "Action": "sts:AssumeRole",
                        "Effect": "Allow",
                        "Principal": {
                            "AWS": "arn:aws:iam::RGW59183818904979875:user/polaris/catalog/admin"
                        }
                    }
                ],
                "Version": "2012-10-17"
            },
            "Description": "",
            "MaxSessionDuration": 3600
        }
    ]
}
----

The Role Policy that defines what S3 resources that Polaris can Access once it
assumes the Role:

----
# aws --profile polaris-root iam list-role-policies --role-name client
{
    "PolicyNames": [
        "catalog_client_policy"
    ]
}
[root@ceph-workstation-685988cc06f597e7ef15b041 ceph]# aws --profile polaris-root iam get-role-policy --role-name client --policy-name catalog_client_policy
{
    "RoleName": "client",
    "PolicyName": "catalog_client_policy",
    "PolicyDocument": {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": [
                    "s3:*"
                ],
                "Effect": "Allow",
                "Resource": [
                    "arn:aws:s3:::polarisdemo/*",
                    "arn:aws:s3:::polarisdemo"
                ]
            }
        ]
    }
}
----

== Deploy the Lab Analytical Container Stack

== Introduction

With storage and IAM wiring complete, bring the *analytic tier* online.
One Podman‑Compose file spins up four services:

* Polaris control‑plane and Iceberg REST catalog
* Trino worker for ad‑hoc SQL
* Jupyter Lab for Spark notebooks
* Superset for dashboards

=== Deployment with Podman Compose using wrapper script

Open a terminal in the repo root and run the following command to start all our
required services using podman-compose:

[source,bash]
----
/root/scripts/demo.sh containers
----

What happens under the hood:

* The script reads `.compose-aws.env` (written by the Ceph Terraform run) to
  inject your bucket location, endpoint URL, and temporary credentials.
* Executes `podman compose up -d`, which downloads or reuses the container
  images and networks them together, the following container services are
  started on the workstation:

  ** polaris
  ** spark
  ** jupiter
  ** trino
  ** superset

* Polls `http://localhost:8182/healthcheck` until Polaris reports *healthy*.

=== Verification

We can run the `podman ps` command from the terminal to get a list of running containers:
----
# podman ps
CONTAINER ID  IMAGE                                           COMMAND               CREATED      STATUS                PORTS                             NAMES
739ff09d1ca5  quay.io/polaris-catalog/polaris:s3compatible    server polaris-se...  4 hours ago  Up 3 hours            0.0.0.0:8181-8182->8181-8182/tcp  polaris
e788a85cba27  docker.io/bitnami/spark:3.5                     /opt/bitnami/scri...  4 hours ago  Up 4 hours            0.0.0.0:7077->7077/tcp            spark
27b31efeffbe  docker.io/jupyter/pyspark-notebook:spark-3.5.0  start-notebook.py     4 hours ago  Up 4 hours (healthy)  0.0.0.0:8888->8888/tcp            jupiter
911e40d103ee  docker.io/trinodb/trino:latest                  /usr/lib/trino/bi...  3 hours ago  Up 3 hours (healthy)  0.0.0.0:8080->8080/tcp            trino
231de3a2e984  docker.io/apache/superset:latest                /bin/bash -c
  s...        3 hours ago                                     Up 3 hours  0.0.0.0:8088->8088/tcp  superset
----

== Bootstrap a Polaris data catalog via Terraform

=== Introduction
Our FreshGoods pipeline already has **storage** (Ceph buckets) and **compute**
(Spark, Trino, Superset).  What it still lacks is a *brain*—a catalog that knows
*which tables exist, who may touch them, and how credentials are issued*.

That brain is **Polaris**.  In this section you’ll run the `polaris/`
Terraform module to codify the governance rules you just heard in the story:

*Nightly product‑movement CSV drops in → PII masked → morning dashboard out.*

=== What the module builds

* **Catalog `prod`** → points at the `s3://polarisdemo` warehouse bucket.
* **Namespace `prod_ns`** → think database / schema.
* **Iceberg RAW & GOLD tables**
  `products_raw` (ingested CSV) → `products_gold` (curated parquet).
* **Four personas (principals)**
  `admin`, `engineer`, `compliance`, `analyst`.
* **Catalog roles & grants** that enforce least‑privilege:

  | Persona      | Allowed actions |
  |--------------|-----------------|
  | *Engineer*   | read / write **RAW** |
  | *Compliance* | read **RAW** + read / write **GOLD** |
  | *Analyst*    | read **GOLD** |
  | *Admin*      | everything (`catalog_admin`) |

* **Short‑lived OAuth 2 tokens** for each persona, exported as Terraform
  outputs so your notebook, Trino CLI, and Superset pick them up
  automatically—no copy‑pasting secrets.

=== Modify Variables

The directory that contains the Polaris Terraform code is
`/root/terraform/polaris` , the variables file is called `variables.tf`, the
only parameter we need to change is the `s3_role_arn` so that it has our Role
ARN with the account ID included, we can get our role ARN with the following
CLI command:

----
# aws --profile polaris-root iam list-roles | jq .Roles[0].Arn
----

You can then edit the terraform variabled file and use your labs role ARN:

----
# vi variables.tf
variable "s3_role_arn" {
  description = "The AWS IAM role ARN for accessing the S3 storage"
  type        = string
  default     = "arn:aws:iam::RGW59183818904979875:role/polaris/catalog/client"
}
----

The rest of the file variables are fine with the defaults, no need to change them.

=== Run Terraform

Everything Polaris needs is now in place:

* `polaris/terraform.tfvars` was auto‑generated by *demo.sh*
  (warehouse URI, S3 role ARN, endpoint, profile name).
* The Polaris service is healthy at http://localhost:8182.

Open a terminal, change to the module directory, and initialise Terraform:

[source,bash]
----
# cd ~/terraform/polaris 
# terraform init
Terraform has been successfully initialized!
----

(Optional) preview the execution plan:

[source,bash]
----
terraform plan
----

You will see resources such as `polaris_catalog.prod` and
`polaris_table.products_raw` in the _“to add”_ section; nothing is created yet.

Apply the configuration:

[source,bash]
----
terraform apply
----

Terraform will ask for confirmation. Type **yes** and hit ⏎.

On success you will see something like:

----
Apply complete! Resources: 24 added, 0 changed, 0 destroyed.

Outputs:

admin_token = <sensitive>
engineer_token = <sensitive>
compliance_token = <sensitive>
analyst_token = <sensitive>
----

Behind the scenes Terraform has:

* created catalog **prod** and namespace **prod_ns**
* registered empty tables **products_raw** and **products_gold**
* minted four principals with role bindings and grants
* produced OAuth tokens that *demo.sh* already copied into
  `notebooks/tokens.json` and the Trino catalog properties

You are ready to ingest data in the next chapter.

=== Verify

==== Export your Polaris endpoint

In your shell, point at the Polaris host and port you used in Terraform:

[source,bash]
----
export POLARIS_HOST=localhost
export POLARIS_PORT=8181
----

====  Obtain a demo user token

We’ll use the “engineer” token for this example (you can repeat for any persona):

[source,bash]
----
cd /root/terraform/polaris
export DEMO_TOKEN=$(terraform output -raw engineer_token)
echo $DEMO_TOKEN | head -c 20; echo "…"
----

If you see a long base64‑style string, you’re good.

====  List all tables in prod/prod_ns

Now call the REST API to list Iceberg tables in your `prod/prod_ns` namespace:

[source,bash]
----
curl -sS \
  -H "Authorization: Bearer $DEMO_TOKEN" \
  -H "Accept: application/json" \
  "http://$POLARIS_HOST:$POLARIS_PORT/api/catalog/v1/prod/namespaces/prod_ns/tables" \
| jq .
----

== Data pipeline execution with Jupiter Notebook 

In this step, you’ll use a Jupyter Notebook to drive our Spark‑based data pipeline end‑to‑end.  Notebooks give us an interactive environment—combining documentation, code, and live output—so you can explore, validate, and debug each stage of the pipeline as you go.  Spark’s built‑in integration with Iceberg makes it trivial to read and write our RAW and GOLD tables with just a few lines of code.

First, we’ll execute a helper script to get the jupyter URL that we will use.  Then you’ll open the `polaris_data_pipeline.ipynb` notebook and run through the ingestion, curation, and validation steps for our FreshGoods demo.

Run the helper script to print your JupyterLab URL and access token:

[source,bash]
----
bash /root/scripts/show_jupyter_notebook_url.sh
----

Copy the printed URL (including the `?token=…` query) into your VNC Desktop browser’s address bar.
  You should see the JupyterLab interface shortly.

In the left sidebar, navigate to the `notebooks/` directory and click on `polaris_data_pipeline.ipynb` to open it.

Follow the notebook cells in order.  Each cell contains explanatory markdown along with the Spark‑SQL or DataFrame APIs to:

  * Ingest the `products_raw_200.csv` file into your Iceberg RAW table
  * Transform, hash PII, and overwrite the Parquet GOLD table
  * Verify and preview pipeline output as the Analyst persona
  * (Bonus) Validate that unauthorized personas cannot see or write data they shouldn’t

By the end of this notebook you will have run a full end‑to‑end Spark data pipeline—demonstrating raw ingest, fine‑grained RBAC, and PII protection—all within an interactive, repeatable environment.


== Access the same Data Pipeline Tables with Different Query Engines (Trino)

Thanks to Polaris’s REST‑based Iceberg catalog, you can point **any** SQL engine at the same tables and get the **exact** same schema, data, and fine‑grained access controls. In this section, we’ll use the Trino CLI against our `prod` catalog and `prod_ns` schema—running as the **Engineer** persona—to:

. Find “Soda” duplicates in the RAW table
. Delete the extra rows
. Confirm the duplicates are gone

This demonstrates how you get consistent governance across compute engines.

=== 1. Connect to Trino CLI

Make sure you have run at least once the helper script `/root/scripts/show_jupiter_notebook_url.sh `. Then launch:

[source,bash]
----
bash /root/lakehouse/trino-cli.sh
----

You should see a prompt like the following, using this prompt we will run our querys:

====
trino:prod_ns>
====

=== Find duplicate “Soda” rows in the RAW table

Now switch to the RAW table and look for any products named “Soda” that appear more than once:

[source,bash]
----
SELECT
  product_id,
  product_name,
  category,
  price,
  quantity,
  COUNT(*) AS occurrences
FROM products_raw
WHERE product_name = 'Soda'
GROUP BY
  product_id,
  product_name,
  category,
  price,
  quantity
HAVING COUNT(*) > 1;
----

If any duplicates exist, you’ll see one or more rows with `occurrences > 1`.

=== Delete the extra duplicates

Keep the earliest timestamped row and delete the rest. Run:

[source,bash]
----
DELETE FROM products_raw
 WHERE (product_id, product_name, category, price, quantity, timestamp)
   IN (
     SELECT product_id,
            product_name,
            category,
            price,
            quantity,
            timestamp
       FROM (
         SELECT
           product_id,
           product_name,
           category,
           price,
           quantity,
           timestamp,
           ROW_NUMBER() OVER (
             PARTITION BY product_id,
                          product_name,
                          category,
                          price,
                          quantity
             ORDER BY timestamp
           ) AS rn
         FROM products_raw
       ) AS dup
      WHERE dup.rn > 1
   );
----

Trino will report how many rows were deleted.

=== Verify the duplicates are gone

Run the same “find duplicates” query again; it should now return zero rows:

[source,bash]
----
SELECT
  product_id,
  product_name,
  category,
  price,
  quantity,
  COUNT(*) AS occurrences
FROM products_raw
WHERE product_name = 'Soda'
GROUP BY
  product_id,
  product_name,
  category,
  price,
  quantity
HAVING COUNT(*) > 1;
----

Expected output:

====
(0 rows)
====

At this point you have:

* Ingested raw CSV into Iceberg
* Curated & protected PII in GOLD
* Used Trino to validate and even mutate the RAW data
* Done all of it under the same fine‑grained RBAC rules

This illustrates the power of a unified, governed Iceberg catalog for multi‑engine analytics.



[IMPORTANT]
====
From this point forward, the guide will help you fix the issues one by one. **We recommend that you first try resolving the cluster problems on your own.** If you get stuck, refer to this guide for assistance.
====


[TIP]
====
====
