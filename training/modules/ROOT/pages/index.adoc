//++++
//<link rel="stylesheet"  href="http://cdnjs.cloudflare.com/ajax/libs/font-awesome/3.1.0/css/font-awesome.min.css">
//++++
:icons: font
:source-language: shell
:numbered:
// Activate experimental attribute for Keyboard Shortcut keys
:experimental:
:source-highlighter: pygments
:sectnums:
:sectnumlevels: 6
:toc: left
:toclevels: 4
:sectanchors:

[.title-highlight]
= Welcome to the IBM Storage Ceph ZeroTrust LakeHouse Lab!

== Lab Introduction
In this guided exercise you will construct a Zero‑Trust data‑lakehouse prototype on IBM Storage Ceph Object Storage and verify that it remains secure, auditable, and performant throughout a complete analytics lifecycle. The lab is organised to familiarise you with both the architectural rationale and the practical implementation steps required for a modern, governed lakehouse.

image::ceph_Datalake.png[float="right",role=padded,width=190]

Over an estimated 90 minutes you will:

. Establish the Ceph‑backed landing zone and create a dedicated S3 tenant to isolate analytic workloads.

. Apply fine‑grained access control by issuing time‑bound credentials through Polaris; table‑level policies are enforced directly by Ceph RGW—no proxy, no performance penalty.

. Deploy the analytics stack—Spark for transformation, Trino for interactive SQL, and Superset for visual exploration—using Infrastructure‑as‑Code and container orchestration.

. Execute an end‑to‑end workflow: ingest raw objects, transform them into Iceberg tables, validate data quality, and present results in a dashboard.

. Inspect and verify the security posture at each stage to confirm that Zero‑Trust requirements are met.

Two possible learning paths are available:

*Conceptual first* – Chapters 2 & 3.
Review the governance challenges inherent to lakehouse designs and see how Ceph, Iceberg, and Polaris combine to address them.

*Hands‑on Lab* – Chapter 4 onward.
Proceed directly to the hands-on lab instructions and assemble the environment step by step.

By the conclusion of the lab you will have a working reference implementation and a clear understanding of how Zero‑Trust principles can be applied to large‑scale analytics on IBM Storage Ceph.

[IMPORTANT]
====
This Enablement is not IBM official documentation for Ceph. For the official documentation, please see https://www.ibm.com/docs/en/storage-ceph/8
====

== ZeroTrust Data LakeHouse
image::CephS3Native.png[align=center, role=diagram]
== Why Governance is a complex topic to solve in Lakehouse deployments?
[.lead]
Enterprises increasingly standardise on **Ceph Object Storage** as the
S3-compatible landing zone for analytics.
However, the moment *multiple* compute engines (Spark, Trino, Flink, AI
notebooks) must operate on the *same* objects, a critical governance gap
emerges:
how to impose **database-style, table-level privileges** directly at the
object layer, *without* introducing performance bottlenecks or an explosion of
one-off security plug-ins.
The sections below dissect the problem, evaluates common but inadequate
approaches, and show how Polaris resolves it through credential vending.

=== Core Challenge for Data Lakehouse Access Control

image::Core_challenge.png[align=center, role=diagram]

[cols="25,75",frame=none,grid=rows]
|===
| *Database-style GRANTs*
  (what analysts expect)
| “Give me `GRANT SELECT ON products` like in Postgres or Snowflake.”

| *Storage-level enforcement*
  (what Zero-Trust demands)
| The security team insists the rule lives **at the object layer**, not
  in ten different SQL engines.

| *Direct I/O paths*
  (what performance needs)
| ETL jobs must read objects at wire-speed – no extra proxy hop
  that throttles throughput.
|===

=== Different Approaches that fall short of solving the Access Control Challenge posed by Data Lakehouses

.Kubernetes & Namespace-per-Bucket Silos
image::Namespace.png[align=center, role=diagram]
[%autowidth,role=step]
|===
|*Pattern* |One bucket + one secret _per_ namespace
|*Looks good* |Strong isolation by default
|*Breaks when…* |Team *Purple* must read the *Blue* team’s raw bucket.
|*Result* |Either you copy data (costly) or copy secrets (scary).
|===

.PEP Only Inside Each Engine
image::Engine.png[align=center, role=diagram]

[%autowidth,role=step]
|===
|*Pattern* |Add a Policy Enforcement Point (PEP) plug-in to Trino, Spark, …
|*Looks good* |Fine-grained rules _inside_ that engine
|*Breaks when…* |A new engine arrives – you re-implement the plug-in *again*.
|*Result* | ∞ code paths, inconsistent policy, hard audits.
|===

.PEP at the Reverse-Proxy in Front of the Storage Layer
image::pepproxy.png[align=center, role=diagram]
[%autowidth,role=step]
|===
|*Pattern* |Send every S3 call through a smart proxy that checks ACLs.
|*Looks good* |Central control, classic pattern.
|*Breaks when…* |90 TB/h ingest hits the proxy; now the proxy *is* the bottleneck.
|*Result* |Scale pain + another SPOF in the data path.
|===

=== Polaris Credential Vending — *Table-Level* RBAC without Sidecars

image::CatalogVending.png[align=center, role=diagram]

[cols="30,70",frame=none,grid=none]
|===
|*Source of truth* |Polaris stores every Iceberg **Table** + its GRANT matrix.
|*Ask* |A Spark executor authenticates once to Polaris.
|*Answer* |Polaris returns a *time-bound STS token* whose S3 policy covers
only the tables that executor may touch.
|*Enforce @ Ceph* |RGW evaluates that policy – no extra proxy hop required.
|===

*What you gain*

* **SQL-style grants** (`GRANT SELECT`, `GRANT INSERT`) with table level granularity.
* **Least-privilege tokens** Credentials are short-lived, valid for minutes, not months. And with the least priveledges required
* **Engine-agnostic** – Spark, Trino, Flink all speak the same Iceberg metadata, so one policy fits all, The Icerbeg catalog is the source of truth.
* **Full speed** – the executor streams Parquet directly from RGW, no proxys needed.


== Lab Workflow Overview

[abstract]
In this hands‑on lab you will stand up a miniature—but fully functional—zero‑trust data lake that lives on *Ceph Object Gateway (RGW)* and is governed by the *Polaris* data‑catalog.  
You will automate the infrastructure with Terraform, ingest data with Spark, query it with Trino, and visualise it in Superset—while watching catalog‑level RBAC enforce least‑privilege at every step.

image::add1.png[float="right",role=padded,width=300]

=== Context for our Real Life Use Case: retail‑analytics

Imagine you are the data team for **FreshGoods**, a mid‑size grocery chain that
ships online orders from 40 local stores.  Every night each store uploads a
CSV “drop” to Ceph RGW containing the day’s *product sales* log, the dataset
contains the following columns:

* `product_id` • what was sold
* `category`   • department (dairy, produce, pantry …)
* `price`
* `quantity`
* `email`      • customer loyalty‑card e‑mail (PII we must protect)
* timestamps, etc.

image::logo.png[float="right",role=padded,width=300]

*Your mission in this lab:* turn those ingested CSV raw files into insight the business can
act on during the next morning —without ever letting unauthorised eyes near the PII.

We will walk you through that journey in this lab, compressed into 90 minutes:

. **Ingest (Engineer ➜ Spark)**
  The *data‑engineer* persona lands last night’s CSV into an **Iceberg RAW
  table** `products_raw` using Spark.  Nothing is filtered or masked yet.

. **Curate & protect (Compliance ➜ Spark)**
  The *compliance* persona reads the RAW table, hashes the `email` column,
  calculates a `total = price * quantity`, and overwrites a clean
  **GOLD table** `products_gold`.
  They can still **read** RAW (audit duty) but only *they* can write GOLD.

. **Explore (Analyst ➜ Trino CLI)**
  The *analyst* persona checks row counts and quick aggregations from Trino,
  confirming the overnight ingest ran.

. **Visualise (Analyst ➜ Superset)**
  Finally the analyst refreshes a Superset dashboard showing *Category sales
  vs. previous day*—the chart the merchandisers see at roll‑call every
  morning.  The hashed emails never leave the lake; the analyst never sees PII.

The glue that enforces who can access what is ithe **Polaris** Iceberg Restfull Catalog.

By the end of the lab you’ll have a governed catalog exactly like a real
retailer might run—just shrunk to one bucket, two tables, and four personas so
we can see the whole life‑cycle in a single sitting.

=== Lab Workflow

image::workflow_lab.png[align=center, role=diagram]
. *Login to Ceph Dashboard* – create RGW account & root user.
. *Run Terraform (`ceph/`)* – wire bucket & IAM.
. *Start containers* – Polaris, Trino, Jupyter, Superset in one command.
. *Run Terraform (`polaris/`)* – create catalog, principals, RAW + GOLD tables.
. *Engineer persona* – ingest `products_raw` in a Jupyter notebook.
. *Compliance persona* – mask PII into `products_gold` in a Jupyter notebook.
. *Analyst persona* – query GOLD via Trino and craft a Superset dashboard.


=== Core skills you’ll practise

|===
| Pillar | You’ll learn to…

| *Storage*
| Create Ceph RGW buckets and IAM roles from the dashboard and Terraform.

| *Infrastructure‑as‑Code*
| Bootstrap all catalog objects (principals, grants, tables) with a single `terraform apply`.

| *Data Engineering*
| Use Spark to ingest CSV → Iceberg and to overwrite snapshots safely.

| *Governance / Security*
| Enforce role‑based access with Polaris tokens; watch failures when a role steps outside its lane.

| *Analytics*
| Query the same Iceberg tables from Trino CLI

| *Visualization*
| Act on your dataset and create Graphs with Apache Superset 
|===

=== Estimated time

Around *90 minutes*.

=== Provided for you

* Pre‑deployed IBM Storage Ceph cluster with RGW(Object Storage Endpoint) runnig.  
* Lab repository with Terraform code, notebooks, helper scripts, and a 200‑row sample CSV.  

== Checking the current state of the lab

If you are reading this doc, you should have your IBM Storage Ceph Lab up and running. If that is not the case, please go
to the IBM Storage Ceph Tech-Zone Collection and Order the Lab https://techzone.ibm.com/collection/64b92c8897187f0017773310)[TechZone Lab Access]

We must open a CLI terminal in our workstation machine and sudo to run the
lab commands as the `ROOT` user. The workstation has the required ceph client
RPMs and the CephX admin keys for our Ceph deployment so that
we can run most of the necessary commands for this lab from the workstation.

----
$ sudo -i
# ceph -s
  cluster:
    id:     09f357c6-b8d6-11ef-bbb7-02009a7a348a
    health: HEALTH_OK

  services:
    mon: 4 daemons, quorum ceph-node1-675b5683b75e66c49dc8f254,ceph-node2-675b5683b75e66c49dc8f254,ceph-node3-675b5683b75e66c49dc8f254,ceph-node4-675b5683b75e66c49dc8f254 (age 9h)
    mgr: ceph-node1-675b5683b75e66c49dc8f254.vadpyr(active, since 9h), standbys: ceph-node2-675b5683b75e66c49dc8f254.yuzazl
    osd: 12 osds: 12 up (since 9h), 12 in (since 9h)
    rgw: 1 daemon active (1 hosts, 1 zones)

  data:
    volumes: 1/1 healthy
    pools:   9 pools, 465 pgs
    objects: 250 objects, 456 KiB
    usage:   856 MiB used, 119 GiB / 120 GiB avail
    pgs:     465 active+clean

  io:
    client:   85 B/s rd, 0 op/s rd, 0 op/s wr
----

From the previous command we can verify that we have a healthy running Ceph Cluster amd it has RGW(S3 endpoint) deployed and active

== Creating the required S3 IAM Account and Root Account User

Before Polaris (or any other service) can create time‑bound STS tokens,
the Ceph RGW must know **which tenant it belongs to** and **who the root
identity is** for that tenant.
In Ceph terminology:

* an IAM **account** ≈ a logical tenant
* an account root **user** ≈ The root account user is the admin of the tenant

We’ll first create the *analytics* tenant, then a *root* user inside it,
and finally an initial S3 bucket that Polaris will use in later steps.

=== Create the *analytics* tenant

The command below runs on our Desktop hosts:

[source,shell]
----
radosgw-admin account create --account-name=analytics
----

*What it does*

* Writes a new tenant record named `analytics` into the RGW metadata store
* Returns an **Account ID** (keep it; we need it for the next step)

=== Create the root user for that tenant

Replace `RGW59183818904979875` with the Account ID you got above:

[source,shell]
----
radosgw-admin user create \
  --uid=analytics_root \
  --display-name=root_analytics_user \
  --account-id=RGW59183818904979875 \
  --account-root \
  --access-key=demo \
  --secret-key=demo
----

*What it does*

* Adds a new root user `analytics_root` **inside** the *analytics* tenant
* Marks it as the **tenant root**, meaning it can create more users and buckets
* Hard‑codes an S3 access‑key / secret‑key pair (`demo / demo`) for lab
  convenience (never do this in production!)

You should see JSON output showing the UID, Account‑ID, and the two keys.

=== Create the **polarisdemo** bucket

Now that we have credentials, we can use the standard AWS CLI

[source,shell]
----
aws --profile polaris-root s3 mb s3://polarisdemo
----

*What it does*

* Places the bucket in the *analytics* tenant because the access key we used
  belongs to the tenant root user
* Provides a clean, empty location where Polaris will write Iceberg tables
  later in the workshop


== Configure and Run the Terraform Automation Code to Create Required Ceph RGW resources

Before we launch Spark, Trino, or Polaris we need a secure *landing zone* inside Ceph’s Object Gateway (RGW).
Rather than clicking through the Ceph Dashboard by hand, we’ll declare every bucket, user, and role in **Terraform**—an open-source “Infrastructure as Code” (IaC) tool that turns cloud resources into version-controlled files.

image::add2.png[center,600]

=== Why automate this step?

* **Consistency & repeatability** – Everyone in the team provisions the *exact* same resources , every time, with a single command.
* **Idempotence** – Running `terraform apply` tomorrow makes zero changes unless you changed the code.
* **Auditability** – All security-sensitive artifacts (bucket names, IAM policies, ARNs) can live in Git—no tribal knowledge locked in a UI click-path.

=== What the code does
[%header,cols="30,~"]
|===
| Block | Purpose

| *Variables (`*.tf` `variable` blocks)*
| Collect user-specific inputs such as the Ceph S3/STS endpoint, the credentials profile that can talk to RGW, and the bucket name that will back the Polaris catalog.

| *AWS provider configured for Ceph*
| Uses the standard `hashicorp/aws` provider but points its `s3`, `sts`, and `iam` endpoints to your Ceph cluster, and forces path-style S3 URLs so they work with RGW.

| *Bucket (data or resource)*
| Looks up—or optionally creates—the S3 bucket named in `var.bucket_name`.  The code is written with `data "aws_s3_bucket"` so it *reads* an already-provisioned bucket, but you can uncomment the `resource "aws_s3_bucket"` block to have Terraform create it instead.

| *IAM user `polaris/catalog/admin`*
| Creates a programmatic user that owns the catalog. Terraform outputs its *access key* and *secret key* so the next module (Polaris) can authenticate.

| *IAM role `polaris/catalog/client`*
| A role that the polaris catalogs assumes via `sts:AssumeRole` to vend a token
to the Query Engine(Spark, Trino) asking for access to a Table. It contains a single inline policy (`catalog_client_policy`) granting **only** `s3:*` on your warehouse bucket.  Principle of least privilege in action.

| *Outputs*
| After `terraform apply` you get:
  * `bucket_arn` – ARN of the warehouse bucket
  * `account_arn` – Ceph pseudo-account ID (used in later trust policies)
  * `location` – `s3://…` URI Polaris will register as its warehouse
  * `role_arn` – ARN of the client role
  * `admin_access_key` / `admin_secret_key` – keys for the admin user (the secret is marked *sensitive* so Terraform hides it in plan logs)
|===


=== Modify Variables
The Ceph Terraform Variables file we need to edit is located in our desktop
machine at `/root/terraform/ceph` with the name `terraform.tfvars`.

You only need to modify the RGW Account ID to match te Account ID you created
on your LAB Environment, the rest of the variables are already filled in for
you.

from the CLI you can get your Account ID with:

```
# radosgw-admin account list
[
    "RGW59183818904979875"
]
```

Then edit the `/root/terraform/ceph/terraform.tfvars` and modify the `account_arn`:

----
# vi /root/terraform/ceph/terraform.tfvars
# Ceph object-gateway (RGW) HTTPS endpoint, used for S3 **and** STS/IAM calls
ceph_endpoint       = "http://ceph-node2"

# Where Terraform’s AWS provider will read your access-key/secret-key pair
credentials_path    = "~/.aws/credentials"
credentials_profile = "polaris-root"

# Name of the bucket that will become Polaris’ warehouse
bucket_name         = "polarisdemo"

# The numerical “account ID” that Ceph assigns when you ran `radosgw-admin account create`
account_arn         = "RGWXXXXXXXXXXXXX"  <<----- Modify this one!

# Object-storage URI the Polaris container should treat as its warehouse
location            = "s3://polarisdemo"
----


=== Run Terraform
With `terraform.tfvars` edited, you are ready to execute the automation.
All commands below assume you are **already on the lab workstation** and that
the code lives in `/root/terraform/ceph`.

[NOTE]
====
If you have never used Terraform before, think of the workflow as:

. *init* – download plugins and build a `.terraform` working directory
. *plan* – show what will change (dry‑run)
. *apply* – make it so (and save state in `terraform.tfstate`)
====

Open a terminal on the lab workstation and change to the module directory:

----
# cd /root/terraform/ceph
----

Initialise the working directory (runs once per clone):

----
# terraform init
----

Terraform downloads the **hashicorp/aws** provider, points it to your Ceph
endpoints, and prints *“Terraform has been successfully initialized!”* when
ready.

Preview the changes (optional but recommended):

----
# terraform plan
----

You should see something like `Plan: 5 to add, 0 to change, 0 to destroy.`
Nothing is created yet—this is just a dry‑run so you can double‑check the
bucket name and account ID.

Apply the configuration:

----
# terraform apply
----

Terraform re‑computes the plan and asks for confirmation.
Type `yes` (or add `-auto-approve` to skip the prompt) and watch the resources
appear.

When the run finishes you will see output similar to:

[source,plain]
----
Apply complete! Resources: 5 added, 0 changed, 0 destroyed.

Outputs:

account_arn      = "RGW59183818904979875"
admin_access_key = "POLARISADMINACCESSKEY"
admin_secret_key = (sensitive value)
bucket_arn       = "arn:aws:s3:::polarisdemo"
location         = "s3://polarisdemo"
role_arn         = "arn:aws:iam::RGW59183818904979875:role/polaris/catalog/client"
----

==== What just happened?
* An S3 bucket (`polarisdemo`) was confirmed (or created) in our Ceph Cluster.
* An IAM user `polaris/catalog/admin` and its access keys were generated inside
  our IAM Account.
* A least‑privilege IAM role `polaris/catalog/client` with an inline S3 policy
  was created.
* Terraform wrote the resource IDs and ARNs to `terraform.tfstate` and echoed
  the key ones as outputs.

=== Verify 

From the terminal we can do a quick verification of the newly created Ceph
Resources:

Bucket:

----
# aws --profile polaris-root s3 ls
2025-06-24 08:57:39 polarisdemo
----

The User that polaris will use to assume the role:

----
# aws --profile polaris-root iam list-users
{
    "Users": [
        {
            "Path": "/polaris/catalog/",
            "UserName": "admin",
            "UserId": "a193f75b-3b62-4996-b8a2-5ba89161ddb2",
            "Arn": "arn:aws:iam::RGW59183818904979875:user/polaris/catalog/admin",
            "CreateDate": "2025-06-24T10:01:58.283604Z"
        }
    ]
}
----

The Role that Polaris will assume to get access to the S3 Resources:

----
# aws --profile polaris-root iam list-roles
{
    "Roles": [
        {
            "Path": "/polaris/catalog/",
            "RoleName": "client",
            "RoleId": "e8596597-1a55-4a44-9b20-364c0682a3a7",
            "Arn": "arn:aws:iam::RGW59183818904979875:role/polaris/catalog/client",
            "CreateDate": "2025-06-24T10:01:58.286Z",
            "AssumeRolePolicyDocument": {
                "Statement": [
                    {
                        "Action": "sts:AssumeRole",
                        "Effect": "Allow",
                        "Principal": {
                            "AWS": "arn:aws:iam::RGW59183818904979875:user/polaris/catalog/admin"
                        }
                    }
                ],
                "Version": "2012-10-17"
            },
            "Description": "",
            "MaxSessionDuration": 3600
        }
    ]
}
----

The Role Policy that defines what S3 resources that Polaris can Access once it assumes the Role:

----
# aws --profile polaris-root iam list-role-policies --role-name client
{
    "PolicyNames": [
        "catalog_client_policy"
    ]
}
# aws --profile polaris-root iam get-role-policy --role-name client --policy-name catalog_client_policy
{
    "RoleName": "client",
    "PolicyName": "catalog_client_policy",
    "PolicyDocument": {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Action": [
                    "s3:*"
                ],
                "Effect": "Allow",
                "Resource": [
                    "arn:aws:s3:::polarisdemo/*",
                    "arn:aws:s3:::polarisdemo"
                ]
            }
        ]
    }
}
----

== Deploy the Lab Analytical Container Stack

image::add5.png[float="right",role=padded,width=400]

=== Introduction

With storage and IAM wiring complete, bring the *analytic compute tier* online.
One Podman‑Compose file spins up four services:

* Polaris control‑plane and Iceberg REST catalog
* Trino worker for ad‑hoc SQL
* Jupyter Lab for Spark notebooks
* Superset for dashboards

=== Deployment with Podman Compose using wrapper script

Open a terminal in the repo root and run the following command to start all our
required services using podman-compose:

----
/root/scripts/demo.sh containers
----

What happens under the hood:

* The script reads `.compose-aws.env` (written by the Ceph Terraform run) to
  inject your bucket location, endpoint URL, and temporary credentials.
* Executes `podman compose up -d`, which downloads or reuses the container
  images and networks them together, the following container services are
  started on the workstation:

  ** polaris
  ** spark
  ** jupyter
  ** trino
  ** superset

* Polls `http://localhost:8182/healthcheck` until Polaris reports *healthy*.

=== Verification

We can run the `podman ps` command from the terminal to get a list of running containers:

----
# podman ps
CONTAINER ID  IMAGE                                           COMMAND               CREATED      STATUS                PORTS                             NAMES
739ff09d1ca5  quay.io/polaris-catalog/polaris:s3compatible    server polaris-se...  4 hours ago  Up 3 hours            0.0.0.0:8181-8182->8181-8182/tcp  polaris
e788a85cba27  docker.io/bitnami/spark:3.5                     /opt/bitnami/scri...  4 hours ago  Up 4 hours            0.0.0.0:7077->7077/tcp            spark
27b31efeffbe  docker.io/jupyter/pyspark-notebook:spark-3.5.0  start-notebook.py     4 hours ago  Up 4 hours (healthy)  0.0.0.0:8888->8888/tcp            jupyter
911e40d103ee  docker.io/trinodb/trino:latest                  /usr/lib/trino/bi...  3 hours ago  Up 3 hours (healthy)  0.0.0.0:8080->8080/tcp            trino
231de3a2e984  docker.io/apache/superset:latest                /bin/bash -c
  s...        3 hours ago                                     Up 3 hours  0.0.0.0:8088->8088/tcp  superset
----

== Bootstrap a Polaris data catalog via Terraform

=== Introduction
Our FreshGoods pipeline already has **storage** (a Ceph bucket `polarisdemo`) and **compute**
(Spark, Trino, Superset) Containers runing.  What it still lacks is a
*brain/source of truth* —a catalog that knows *which tables exist, who may modify them, and how credentials are issued*.


That brain is **Polaris**.  In this section you’ll run the `polaris/`
Terraform module to automate the governance rules for our Example Data
Pipeline:

*Nightly batch of shops product‑movement drops in CSV format → All PII is
masked → in the morning data dashboard visualizations are available for the
stakeholders to take action.*

image::add2.png[float="right",role=padded,width=300]


=== What the module builds

* **Catalog `prod`** → points at the `s3://polarisdemo` warehouse bucket.
* **Namespace `prod_ns`** → think database / schema.
* **Iceberg RAW & GOLD tables**
  `products_raw` (ingested CSV Table) → `products_gold` (anonymized and curated parquet table).
* **Four personas (principals)**
  `admin`, `engineer`, `compliance`, `analyst`.
* **Catalog roles & grants** that enforce least‑privilege:

  | Persona      | Allowed actions |
  |--------------|-----------------|
  | *Engineer*   | read / write **RAW** |
  | *Compliance* | read **RAW** + read / write **GOLD** |
  | *Analyst*    | read **GOLD** |
  | *Admin*      | everything (`catalog_admin`) |

* Authentication tou the catalog happens with **Short‑lived OAuth 2 tokens** for each persona, exported as Terraform
  outputs so your different Query Engines: notebook, Trino CLI, and Superset pick them up automatically—no copy‑pasting secrets.

=== Modify Variables

In our Desktop the directory that contains the Polaris Terraform code is
`/root/terraform/polaris` , the variables file is called `variables.tf`, the
only parameter we need to change is the `s3_role_arn` so that it has our Role
ARN with the account ID included, we can get our role ARN with the following
RGW admin CLI command:

----
# aws --profile polaris-root iam list-roles | jq .Roles[0].Arn
----

You can then edit the terraform variabled file and use your labs role ARN:

----
# vi variables.tf
variable "s3_role_arn" {
  description = "The AWS IAM role ARN for accessing the S3 storage"
  type        = string
  default     = "arn:aws:iam::RGWXXXXXXXXXXXXXXXX:role/polaris/catalog/client" < --- MODIFY HERE
}
----

The rest of the file variables are fine with the defaults, no need to change them.

=== Run Terraform

Everything Polaris needs is now in place: Open a terminal, change to the module directory, and initialise Terraform:

----
# cd ~/terraform/polaris 
# terraform init
Terraform has been successfully initialized!
----

Apply the configuration:

----
terraform apply
----

Terraform will ask for confirmation. Type **yes** and hit ⏎.

On success you will see something like:

----
Apply complete! Resources: 24 added, 0 changed, 0 destroyed.

Outputs:

admin_token = <sensitive>
engineer_token = <sensitive>
compliance_token = <sensitive>
analyst_token = <sensitive>
----

Behind the scenes Terraform has:

* created catalog **prod** and namespace **prod_ns**
* created and registered two empty tables **products_raw** and
  **products_gold** with their schemas
* minted four principals(users) with role bindings and grants
* produced OAuth tokens for our Users that our Query Engines will consume

You are ready to ingest data in the next chapter.

=== Verify

==== Export your Polaris endpoint

In your shell, point at the Polaris host and port you used in Terraform:

----
export POLARIS_HOST=localhost
export POLARIS_PORT=8181
----

====  Obtain a demo user token

We’ll use the “engineer” token for this example (you can repeat for any persona):

----
cd /root/terraform/polaris
export DEMO_TOKEN=$(terraform output -raw engineer_token)
echo $DEMO_TOKEN
----

If you see a long base64‑style string, you’re good.

====  List all tables in prod/prod_ns

Now call the REST API to list Iceberg tables in your `prod/prod_ns` namespace:

[source,bash]
----
curl -sS \
  -H "Authorization: Bearer $DEMO_TOKEN" \
  -H "Accept: application/json" \
  "http://$POLARIS_HOST:$POLARIS_PORT/api/catalog/v1/prod/namespaces/prod_ns/tables" \
| jq .
----

== Data pipeline execution with Jupyter Notebook 

In this step, you’ll use a Jupyter Notebook to drive our Spark‑based data pipeline end‑to‑end.  Notebooks give us an interactive environment—combining documentation, code, and live output—so you can explore, validate, and debug each stage of the pipeline as you go.  Spark’s built‑in integration with Iceberg makes it trivial to read and write our RAW and GOLD tables with just a few lines of code.

First, we’ll execute a helper script to get the jupyter URL that we will use.  Then you’ll open the `polaris_data_pipeline.ipynb` notebook and run through the ingestion, curation, and validation steps for our FreshGoods demo.

Run the helper script to print your JupyterLab URL and access token:

[source,bash]
----
bash /root/scripts/show_jupiter_notebook_url.sh
----

Copy the printed URL (including the `?token=…` query) into your VNC Desktop browser’s address bar.
  You should see the JupyterLab interface shortly.

image::jup1.png[jupiter1,1800]

In the left sidebar, navigate to the `notebooks/` directory and click on `polaris_data_pipeline.ipynb` to open it.

image::jup2.png[jupiter2,1800]

Follow the notebook cells in order.  Each cell contains explanatory markdown along with the Spark‑SQL or DataFrame APIs to:

  * Ingest the `products_raw_200.csv` file into your Iceberg RAW table
  * Transform, hash PII, and overwrite the Parquet GOLD table
  * Verify and preview pipeline output as the Analyst persona
  * (Bonus) Validate that unauthorized personas cannot see or write data they shouldn’t

You can run cells one by one using the > button, and  waiting to get the output before running the next cell:

image::jup3.png[jupiter3,1800]

By the end of this notebook you will have run a full end‑to‑end Spark data pipeline—demonstrating raw ingest, fine‑grained RBAC, and PII protection—all within an interactive, repeatable environment.

== Access the same Data Pipeline Tables with Different Query Engines (Trino)

Thanks to Polaris’s REST‑based Iceberg catalog, you can point **any** Icerberg Compatible SQL engine at the same tables and get the **exact** same schema, data, and fine‑grained access controls. 
image::add3.png[float="right",role=padded,width=300]

In this section, we’ll use the Trino CLI against our `prod` catalog and `prod_ns` schema—running as the **Engineer** persona—to:

. Find “Soda” duplicates in the RAW table
. Delete the extra rows
. Confirm the duplicates are gone

This demonstrates how you get consistent governance to the centralized datasets stored in Ceph across compute engines.


=== Connect to Trino CLI

Make sure you have run at least once the helper script
`/root/scripts/show_jupiter_notebook_url.sh ` from your Desktop host terminal. Then launch:

[source,bash]
----
bash /root/lakehouse/trino-cli.sh
----

You should see a prompt like the following, using this prompt we will run our
Trino SQL queries:

----
trino:prod_ns>
----

=== Find duplicate “Soda” rows in the RAW table

We will run a SQL query that looks for any products named “Soda” that appear
more than once in the `products_raw` table.

[source,bash]
----
SELECT
  product_id,
  product_name,
  category,
  price,
  quantity,
  COUNT(*) AS occurrences
FROM products_raw
WHERE product_name = 'Soda'
GROUP BY
  product_id,
  product_name,
  category,
  price,
  quantity
HAVING COUNT(*) > 1;
----

If any duplicates exist, you’ll see one or more rows with `occurrences > 1`.

=== Delete the extra duplicates

[NOTE]
====
`DELETE FROM` is available only when the Iceberg table is created in
*format‑version 2* (Iceberg v2).
The `products_raw` table you generated earlier was written by Spark with
`iceberg.format-version = 2`, which is why the command works.
If you attempt the same statement on a v1 table Trino will return
`NOT_SUPPORTED: Cannot delete from non‑transactional table`.
====

Keep the earliest timestamped row and delete the rest. Run:

[source,bash]
----
DELETE FROM products_raw
 WHERE (product_id, product_name, category, price, quantity, timestamp)
   IN (
     SELECT product_id,
            product_name,
            category,
            price,
            quantity,
            timestamp
       FROM (
         SELECT
           product_id,
           product_name,
           category,
           price,
           quantity,
           timestamp,
           ROW_NUMBER() OVER (
             PARTITION BY product_id,
                          product_name,
                          category,
                          price,
                          quantity
             ORDER BY timestamp
           ) AS rn
         FROM products_raw
       ) AS dup
      WHERE dup.rn > 1
   );
----

Trino will report how many rows were deleted.

=== Verify the duplicates are gone

Run the same “find duplicates” query again; it should now return zero rows:

[source,bash]
----
SELECT
  product_id,
  product_name,
  category,
  price,
  quantity,
  COUNT(*) AS occurrences
FROM products_raw
WHERE product_name = 'Soda'
GROUP BY
  product_id,
  product_name,
  category,
  price,
  quantity
HAVING COUNT(*) > 1;
----

Expected output:

----
(0 rows)
----

At this point you have:

* Ingested raw CSV into Iceberg
* Curated & protected PII in GOLD
* Used Trino to validate and even mutate the RAW data
* Done all of it under the same fine‑grained RBAC rules

This illustrates the power of a unified, governed Iceberg catalog for multi‑engine analytics.

== Using Apache Superset to build visualizations to gain insights into our data

image::add4.png[intro,900]
=== Goal
Create a bar chart that shows the ten products with the highest revenue and
place it on the existing *Sales Overview* dashboard.

[NOTE]
====
Log in to Superset with **user =`admin` / password =`admin`**.
The container resets these credentials every time it (re)starts.
====

===  Query the data in **SQL Lab**

Navigate to menu:SQL[SQL Lab → SQL Editor].
In the *Database* drop‑down, choose **Trino (Iceberg)**. +
Paste the query and click the *Run* ▶ button.

[source,sql]
----
SELECT
    product_name,
    SUM(total) AS revenue
FROM prod_ns.products_gold
GROUP BY product_name
ORDER BY revenue DESC
LIMIT 10;
----

Verify you get exactly 10 rows in the results panel.

image::superset1.png[Query results,1800]

===  Save the query as a dataset

Click the *Save* button above the results → **Save as dataset**.

image::superset2.png[Save dataset dialog,1800]

Fill in:

  * *Dataset name* :: `top_revenue_products`

Press btn:[Save and Explore]. You will see a toast “Dataset saved”.

image::Superset3.png[Save Dataset,1800]

===  Explore and build the chart

Superset opens the Chart Builder with the dataset already selected.

In the *Choose chart type* gallery, click **Bar Chart**.

image::Superset4.png[Pie Chart,1800]

==== Configure the *Data* tab

|===
|Control |Value |How

|*X‑axis* |`product_name` |Drag the column from the left column list.
|*Metrics* |`revenue` |Drag the metric; remove `COUNT(*)` if present.
|*Sort by* |`revenue` ↓ (descending)|Drag `revenue` to the field; keep Desc.
|*Row limit* |`10` |Optional—keeps it to top‑10.
|===

Click btn:[Update chart]
image::Superset5.png[Populated Data tab,1800]

==== Tidy up in the *Customize* tab (optional)

* *Y‑axis title* select: `Revenue ($)`
* *Y‑axis format* select:  `$.2f`
* Pick a colour scheme you like.

Click btn:[Update chart] again to preview.

image::superset6.png[Advanced,1800]

=== Save the chart and add to a dashboard

Click btn:[Save].
Fill in:
  * *Chart name* :: `Top‑10 Products by Revenue`
  * *Add to dashboard* ::
    ** If *Sales Overview* exists → choose it.
    ** Otherwise → **+ New dashboard** → `Sales Overview`.
Click btn:[Save & Go to dashboard].

image::superset7.png[Advanced,1800]

Superset redirects you to the dashboard in *Edit* mode with your new bar chart already placed. Resize or drag to the desired position, then click btn:[Save] in the dashboard header.

image::Superset8.png[Dashboard with new chart,1800]

You’ve successfully added an interactive visual to your dashboard using nothing but SQL Lab and the chart builder—well done!

