#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

services:
  polaris:
    image: quay.io/polaris-catalog/polaris:s3compatible
    ports:
      - "8181:8181"
      - "8182:8182"
    env_file: .compose-aws.env
    environment:
      POLARIS_BOOTSTRAP_CREDENTIALS: default-realm,root,s3cr3t
      polaris.realm-context.realms: default-realm
      quarkus.otel.sdk.disabled: "true"
    container_name: polaris

  trino:
    image: docker.io/trinodb/trino:latest
    container_name: trino
    depends_on:
      - polaris
    ports:
      - "8080:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog
    environment:
      - JAVA_TOOL_OPTIONS=-Duser.timezone=UTC
      - HTTP_SERVER_AUTHENTICATION_ALLOW_INSECURE_OVER_HTTP=true
  spark:
    image: docker.io/bitnami/spark:3.5
    depends_on:
      - polaris
    ports: 
#      - 8080:8080  # Master Web UI
      - 7077:7077  # Master Port
    environment:
      SPARK_SUBMIT_OPTIONS: >
        --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,org.apache.hadoop:hadoop-aws:3.4.0
    env_file: .compose-aws.env 
    container_name: spark
    volumes:
      - ./spark/conf/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro

  jupyter:
    image: docker.io/jupyter/pyspark-notebook:spark-3.5.0
    container_name: jupyter
    depends_on:
      - spark
    ports:
      - "8888:8888"
    env_file: .compose-aws.env
    environment:
      SPARK_OPTS: >
        --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,
                  org.apache.hadoop:hadoop-aws:3.4.0
    volumes:
      - ./notebooks:/home/jovyan/work
    container_name: jupiter

  superset:
    build:
      context: .
      dockerfile: ./superset/Dockerfile
    container_name: superset
    depends_on:
      - trino
    ports:
      - "8088:8088"

    # load any AWS/S3 creds first
    env_file:
      - .compose-aws.env

    # *then* explicitly set our Superset admin creds
    environment:
      SUPERSET_SECRET_KEY: "oh-so-secret"
      SUPERSET_LOAD_EXAMPLES: "no"
      ADMIN_USERNAME:    "admin"
      ADMIN_FIRST_NAME:  "Admin"
      ADMIN_LAST_NAME:   "User"
      ADMIN_EMAIL:       "admin@example.com"
      ADMIN_PASSWORD:    "admin"

    volumes:
      - superset_home:/home/superset
      - ./superset:/app/import

    command: >
      /bin/bash -eux -c '
        # 1) make sure the DB schema is up-to-date
        superset db upgrade &&

        # 2) init the internal state & create the admin user
        superset init &&
        superset fab create-admin \
          --username admin \
          --firstname Admin \
          --lastname  Admin2 \
          --email     admin@admin.com \
          --password  admin &&
        uv pip install --no-cache-dir sqlalchemy-trino trino
        superset import-datasources -p /app/import/superset_dataset.zip  --username admin
        superset import-dashboards -p /app/import/superset_dash.zip --username admin
        # 3) finally launch the web server
        superset run -h 0.0.0.0 -p 8088

      '
volumes:
  superset_home: {}

